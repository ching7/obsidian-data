---
created: '2025-05-28 15:37:31'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/BJv5dDo3OoeOYlxFvtqcRwJnnvf
modified: '2025-07-07 17:54:44'
source: feishu
title: 两地三中心方案-2025-05-28
---

两地三中心方案-2025-05-28
1、简介
1.1、目标
本方案旨在说明服务机器人产品平台V1.1 的两地三中心架构、实现方案和测试方案，确保系统能够在各种故障场景下满足业务连续性要求，为企业级应用提供可靠的数据存储和服务保障，达成立项目标：支持两地三中心灾备方案（同城应用双活，同城数据热备，异地数据冷备），RTO<24 小时，RPO<120 分钟
1.2、术语解释和名词约定


缩写、术语

解释

DocQA

星火大模型知识问答平台

RPO

Recovery Point Objective，灾难发生后允许丢失的数据量（以时间度量）

RTO

Recovery Time Objective，灾难造成的故障或瘫痪状态恢复到可正常运行状态所需的时间

中心A

主生产中心

中心B

同城灾备中心

中心C

异地灾备中心

1.3、参考资料
《202410-MySQL异地容灾方案_v2_CIJP-3F0Q-BJ00-SPE4.pptx》

2、两地三中心实现方案
2.1、整体架构
image.png

2.2、应用层改造点梳理

2.2、mysql两地三中心方案
MySQL 两地三中心部署文档
2.3、elasticsearch两地三中心方案
2.3.1、部署架构图
image3.png

架构图说明：
1）logstash与elasticsearch版本要对应上，同为8.4.1版本
2）es下载地址：https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.4.1-linux-x86_64.tar.gz
3）logstash下载地址：https://artifacts.elastic.co/downloads/logstash/logstash-8.4.1-linux-x86_64.tar.gz
4）正常情况下，同城主往同城备写（不往异地写，是为了减轻同城主压力），同城备往异地写。当同城主宕机恢复时，同城备或异地可以添加同城为目标机器，将数据同步到同城主；当同城备宕机恢复时，同城主可将数据同步到同城备；当异地宕机恢复时，同城备可将数据同步到异地；当同城主和同城备宕机恢复时，异地可将数据同步到同城主和同城备；
2.3.2、部署规划
部署三个中心elasticsearch
1）同城主：
elasticsearch：http://172.30.94.25:9200/            部署用户名/密码：elasticsearch/Es@2024!
Logstash（启动）        源：172.30.94.25            目标：172.29.230.6
2）同城备：
elasticsearch：http://172.29.230.6:9200/            部署用户名/密码：elasticsearch/Es@2024!
Logstash（启动）        源：172.29.230.6            目标：172.30.94.25、172.29.67.75
3）异地：
elasticsearch：http://172.29.67.75:9200/            部署用户名/密码：elasticsearch/Es@2024!
Logstash（停止）        源：172.29.67.75            目标：172.30.94.25、172.29.230.6
2.3.3、部署操作


Plain Text
1.创建普通用户
sudo useradd -m elasticsearch
sudo passwd elasticsearch
# 赋权指定文件夹
sudo chown elasticsearch:elasticsearch/home/elasticsearch/

2.配置环境变量
# 修改/etc/security/limits.conf文件
vim /etc/security/limits.conf
# 末尾添加以下配置
* soft nofile 655360 
* hard nofile 655360 
* soft nproc 655360 
* hard nproc 655360 
* soft core unlimited 
* hard core unlimited 
* soft stack unlimited 
* hard stack unlimited

# 修改/etc/sysctl.conf文件
vim /etc/sysctl.conf
vm.max_map_count=655360
# 重载
sysctl -p

3.JDK安装
# 设置环境变量
vim ~/.bashrc
# 输入i编辑文件，在最后追加JAVA_HOME和path
#注：JAVA_HOME为实际的地址，在这次示例中为/home/iflytek/jdk/jdk-17
export JAVA_HOME=/home/iflytek/jdk/jdk-17
export CLASSPATH=$:CLASSPATH:$JAVA_HOME/lib/
export PATH=$PATH:$JAVA_HOME/bin
# 重载环境变量
source ~/.bashrc
# 验证是否安装成功
java -version

4.部署安装ES
su - elasticsearch
# 创建文件夹，将安装包上传
mkdir /elastic
# 安装上传完后，解压
cd /elastic
tar -zxvf elasticsearch-8.4.1-linux-x86_64.tar.gz

# 修改config/elasticsearch.yml文件
# 同城主机房       合肥B3-5     172.30.94.25 （同城两个机房的应用数据都写入该es）
cluster.name: fwjqr-B35
node.name: B35-node
network.host: 172.30.94.25
http.port: 9200
cluster.initial_master_nodes: ["B35-node"]

# 同城备机房        合肥城市云   172.29.230.6
cluster.name: fwjqr-CSY
node.name: CSY-node
network.host: 172.29.230.6
http.port: 9200
cluster.initial_master_nodes: ["CSY-node"]

# 异地机房          上海临港1   172.29.67.75
cluster.name: fwjqr-SHLG1
node.name: SHLG1-node
network.host: 172.29.67.75
http.port: 9200
cluster.initial_master_nodes: ["SHLG1-node"]

# 根据实际情况调整启动内存，修改jvm.options配置，注意前面不要留空格！！
-Xms4g
-Xmx4g

# 进入es的bin目录，启动ES。   浏览器访问9200端口，查看是否运行成功
./elasticsearch -d

5.部署安装Logstash
首先需要在本地或服务器上安装Logstash。可以从Elastic官网下载对应版本的Logstash并进行安装。
下载Logstash（需跟ES版本一致）
全量下载地址：https://www.elastic.co/cn/downloads/logstash
对应版本下载地址：https://artifacts.elastic.co/downloads/logstash/logstash-8.4.1-linux-x86_64.tar.gz

解压logstash-8.4.1-linux-x86_64.tar.gz版本包
tar -zxvf logstash-8.4.1-linux-x86_64.tar.gz

# 同城主机器部署     同城主->同城备
在Logstash的config目录下创建一个名为logstash-config.conf的配置文件，用于定义数据同步的输入和输出
填写以下配置：
input {
   
  # 配置索引
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.30.94.25:9200"]   
    # 索引
    index => "srp_operate_dialog"
    # 用户名
    # user => "your_username"
    # 密码
    # password => "your_password"
    # 查询语句，查询全部
    query => '{ "query": { "query_string": { "query": "*" } } }'
    schedule => "* * * * *"  # 每分钟执行一次同步
    # 获取元数据，如：_id、_index
    docinfo => true
    docinfo_target => "[@metadata][doc]"
  }      
}

# 配置timestemp时区，按需配置
filter {
  ruby {
    code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"
  }
  ruby {
    code => "event.set('@timestamp', event.get('timestamp'))"
  }
  mutate {
    remove_field => ["timestamp"]
  }
}

output {
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.29.230.6:9200"]
    # 索引
    index => "srp_operate_dialog"
    # user => "your_username"
    # password => "your_password"
    # _id标识唯一，不重复插入数据
    document_id => "%{[@metadata][doc][_id]}"
    action => "update"
    doc_as_upsert => true
    document_type => "_doc" # 根据ES版本选择适当的document_type
  }
  # 打印日志，调试时可打开
  # stdout {
  #   codec => rubydebug
  # }
}

启动logstash
bin/logstash -f config/logstash-config.conf



# 同城备机器部署     同城备->异地
在Logstash的config目录下创建一个名为logstash-config.conf的配置文件，用于定义数据同步的输入和输出
填写以下配置：
input {
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.29.230.6:9200"]
    # 索引
    index => "srp_operate_dialog"
    # 用户名
    # user => "your_username"
    # 密码
    # password => "your_password"
    # 查询语句，查询全部
    query => '{ "query": { "query_string": { "query": "*" } } }'
    schedule => "* * * * *"  # 每分钟执行一次同步
    # 获取元数据，如：_id、_index
    docinfo => true
    docinfo_target => "[@metadata][doc]"
  }
}

# 配置timestemp时区，按需配置
filter {
  ruby {
    code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"
  }
  ruby {
    code => "event.set('@timestamp', event.get('timestamp'))"
  }
  mutate {
    remove_field => ["timestamp"]
  }
}

output {
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.29.67.75:9200"]
    # 索引
    index => "srp_operate_dialog"
    # user => "your_username"
    # password => "your_password"
    # _id标识唯一，不重复插入数据
    document_id => "%{[@metadata][doc][_id]}"
    action => "update"
    doc_as_upsert => true
    document_type => "_doc" # 根据ES版本选择适当的document_type
  }
  # 打印日志，调试时可打开
  # stdout {
  #   codec => rubydebug
  # }
}

启动logstash
bin/logstash -f config/logstash-config.conf



# 异地机器部署     异地->同城主和同城备
在Logstash的config目录下创建一个名为logstash-config.conf的配置文件，用于定义数据同步的输入和输出
填写以下配置：
input {
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.29.67.75:9200"]
    # 索引
    index => "srp_operate_dialog"
    # 用户名
    # user => "your_username"
    # 密码
    # password => "your_password"
    # 查询语句，查询全部
    query => '{ "query": { "query_string": { "query": "*" } } }'
    schedule => "* * * * *"  # 每分钟执行一次同步
    # 获取元数据，如：_id、_index
    docinfo => true
    docinfo_target => "[@metadata][doc]"
  }
}

# 配置timestemp时区，按需配置
filter {
  ruby {
    code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"
  }
  ruby {
    code => "event.set('@timestamp', event.get('timestamp'))"
  }
  mutate {
    remove_field => ["timestamp"]
  }
}

output {
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.30.94.25:9200"]
    index => "srp_operate_dialog"
    # user => "your_username"
    # password => "your_password"
    # _id标识唯一，不重复插入数据
    document_id => "%{[@metadata][doc][_id]}"
    action => "update"
    doc_as_upsert => true
    document_type => "_doc" # 根据ES版本选择适当的document_type
  }
  elasticsearch {
    # 样例为单机，集群用逗号分隔，如：["http://localhost:9200", "http://localhost:9201"]
    hosts => ["http://172.29.230.6:9200"]
    index => "srp_operate_dialog"
    # user => "your_username"
    # password => "your_password"
    # _id标识唯一，不重复插入数据
    document_id => "%{[@metadata][doc][_id]}"
    action => "update"
    doc_as_upsert => true
    document_type => "_doc" # 根据ES版本选择适当的document_type
  }
  # 打印日志，调试时可打开
  # stdout {
  #   codec => rubydebug
  # }
}

不启动logstash ！！！！



多个索引混合场景如何配置，如：同时操作srp_operate_dialog、srp_knowledge_publish索引
input {

  elasticsearch {
    hosts => ["http://172.30.94.25:9200"]
    index => "srp_operate_dialog"
    # user => "your_username"
    # password => "your_password"
    #query => '{"query": {"match_all": {}}}'
    query => '{ "query": { "query_string": { "query": "*" } } }'
    schedule => "* * * * *"  # 每分钟执行一次同步
    docinfo => true
    docinfo_target => "[@metadata][doc]"
  }

  elasticsearch {
    hosts => ["http://172.30.94.25:9200"]
    index => "srp_knowledge_publish"
    # user => "your_username"
    # password => "your_password"
    #query => '{"query": {"match_all": {}}}'
    query => '{ "query": { "query_string": { "query": "*" } } }'
    schedule => "* * * * *"  # 每分钟执行一次同步
    docinfo => true
    docinfo_target => "[@metadata][doc]"
  }

}

filter {
  ruby {
    code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"
  }
  ruby {
    code => "event.set('@timestamp', event.get('timestamp'))"
  }
  mutate {
    remove_field => ["timestamp"]
  }
}

output {
  if [@metadata][doc][_index] == "srp_operate_dialog" {
    elasticsearch {
      hosts => ["http://172.29.230.6:9200"]
      index => "srp_operate_dialog"
      # user => "your_username"
      # password => "your_password"
      document_id => "%{[@metadata][doc][_id]}"
      action => "update"
      doc_as_upsert => true
      document_type => "_doc" # 根据ES版本选择适当的document_type
    }
  }
  if [@metadata][doc][_index] == "srp_knowledge_publish" {
    elasticsearch {
      hosts => ["http://172.29.230.6:9200"]
      index => "srp_knowledge_publish"
      # user => "your_username"
      # password => "your_password"
      document_id => "%{[@metadata][doc][_id]}"
      action => "update"
      doc_as_upsert => true
      document_type => "_doc" # 根据ES版本选择适当的document_type
     }
  }
 stdout {
     codec => rubydebug
 }
}

2.3.4、部署分词
下载地址：https://release.infinilabs.com/analysis-ik/stable/
在es的plugins目录下，建立ik文件
mkdir ik
将下载elasticsearch-analysis-ik-8.4.1.zip分词插件，放入ik文件夹，并解压
unzip elasticsearch-analysis-ik-8.4.1.zip
image4.png


2.3.5、创建索引
执行以下脚本，创建服务机器人索引
[effect.sh]

2.3.6、造数工具
造数demo，测试数据同步效果
1、pom引入


Plain Text
<!-- ES 依赖 -->
<dependency>
    <groupId>co.elastic.clients</groupId>
    <artifactId>elasticsearch-java</artifactId>
    <version>8.8.2</version>
    <exclusions>
        <exclusion>
            <artifactId>jakarta.json-api</artifactId>
            <groupId>jakarta.json</groupId>
        </exclusion>
        <exclusion>
            <artifactId>elasticsearch-rest-client</artifactId>
            <groupId>org.elasticsearch.client</groupId>
        </exclusion>
    </exclusions>
</dependency>
<!-- https://mvnrepository.com/artifact/org.elasticsearch.client/elasticsearch-rest-client -->
<dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>elasticsearch-rest-client</artifactId>
    <version>8.13.3</version>
</dependency>

<dependency>
    <groupId>jakarta.json</groupId>
    <artifactId>jakarta.json-api</artifactId>
    <version>2.1.2</version>
</dependency>
<dependency>
    <groupId>jakarta.json.bind</groupId>
    <artifactId>jakarta.json.bind-api</artifactId>
    <version>3.0.0</version>
</dependency>
注意rest-client和json的版本号，如果不排除，则默认使用的是springboot里面的，所以这里需要先排除，在重新引用，如果重新应用了还是使用的是sprinboot的内置版本，就需要pom强制修改依赖版了。


Plain Text
<properties>
    <jakarta-json.version>2.1.2</jakarta-json.version>
    <elasticsearch.version>8.13.3</elasticsearch.version>
</properties>
2、Java代码


Plain Text
import java.io.IOException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;
import org.apache.http.HttpHost;
import org.elasticsearch.client.RestClient;
import com.iflytek.npvs.dto.ElasticDTO;
import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.ElasticsearchException;
import co.elastic.clients.elasticsearch.core.IndexResponse;
import co.elastic.clients.json.jackson.JacksonJsonpMapper;
import co.elastic.clients.transport.ElasticsearchTransport;
import co.elastic.clients.transport.rest_client.RestClientTransport;

public class EeasticDataTest {

        private static final int CONCURRENT_THREADS = 1000;
        private static final long TOTAL_DOCS = 100000L;
        private static final String INDEX_NAME = "srp_operate_dialog";
        private static final AtomicLong idCounter = new AtomicLong(0);

        public static void main(String[] args) throws ElasticsearchException, IOException {

                // 初始化Elasticsearch客户端
                RestClient restClient = RestClient.builder(new HttpHost("172.29.230.6", 9200, "http")).build();
                ElasticsearchTransport transport = new RestClientTransport(restClient, new JacksonJsonpMapper());
                ElasticsearchClient client = new ElasticsearchClient(transport);

                try {

                        ExecutorService executorService = Executors.newFixedThreadPool(CONCURRENT_THREADS);
                        for (int j = 0; j < CONCURRENT_THREADS; j++) {
                                executorService.submit(() -> {
                                        while (true) {
                                                long id = idCounter.incrementAndGet();
                                                if (id > TOTAL_DOCS) {
                                                        break;
                                                }
                                                // 数据实体
                                                ElasticDTO els = new ElasticDTO();
                                                els.setCallId("test" + id + "_12345678");
                                                els.setAnswer("造数测试");
                                                els.setEsId("test" + id + "_12345678");
                                                // 获取当前时间
                                                LocalDateTime now = LocalDateTime.now();                                        
                                                // 定义日期时间格式
                                                DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd hh:mm:ss");
                                                els.setStartTime(now.format(formatter));
                                                els.setQuestion("造数测试");
                                                els.setRobot("89757");
                                                els.setTenantId("bianhao89757");
                                                els.setQuestion("帮我测试下数据写入效率");
                                                els.setExtension("造数测试");
                                                els.setErrorType("造数测试");
                                                els.setSource("造数测试");
                                                els.setNote("造数测试");                                        

                                                // 往索引写数据
                                                try {
                                                        IndexResponse indexResponse = client.index(i -> i.index(INDEX_NAME)
                                                                        // 设置id
                                                                        .id(els.getCallId())
                                                                        // 传入对象
                                                                        .document(els));
                                                } catch (ElasticsearchException e) {
                                                        // 实际代码需要通过log打印
                                                        e.printStackTrace();
                                                } catch (IOException e) {
                                                        e.printStackTrace();
                                                }
                                        }
                                });
                        }
                } catch (Exception e) {
                        e.printStackTrace();
                }
        }
}



Plain Text
public class ElasticDTO {
        
        private String callId;       
        private String answer;        
        private String errorType;        
        private String esId;        
        private String extension;        
        private String note;        
        private String question;        
        private String robot;        
        private String source;        
        private String startTime;        
        private String tenantId;
        
        public String getCallId() {
                return callId;
        }
        public void setCallId(String callId) {
                this.callId = callId;
        }

        public String getAnswer() {
                return answer;
        }
        public void setAnswer(String answer) {
                this.answer = answer;
        }

        public String getErrorType() {
                return errorType;
        }
        public void setErrorType(String errorType) {
                this.errorType = errorType;
        }

        public String getEsId() {
                return esId;
        }
        public void setEsId(String esId) {
                this.esId = esId;
        }

        public String getExtension() {
                return extension;
        }
        public void setExtension(String extension) {
                this.extension = extension;
        }

        public String getNote() {
                return note;
        }
        public void setNote(String note) {
                this.note = note;
        }

        public String getQuestion() {
                return question;
        }
        public void setQuestion(String question) {
                this.question = question;
        }

        public String getRobot() {
                return robot;
        }
        public void setRobot(String robot) {
                this.robot = robot;
        }

        public String getSource() {
                return source;
        }
        public void setSource(String source) {
                this.source = source;
        }

        public String getStartTime() {
                return startTime;
        }
        public void setStartTime(String startTime) {
                this.startTime = startTime;
        }

        public String getTenantId() {
                return tenantId;
        }
        public void setTenantId(String tenantId) {
                this.tenantId = tenantId;
        }

}
2.3.7、数据同步功能验证
2.3.7.1、单索引 同城主-->同城备    
配置和启动logstash，并设置每分钟同步一次；
image5.png

全量同步验证
1）通过造数demo往elasticsearch同城主机器172.30.94.25写入2条数据；
image6.png

2）观察elasticsearch同城备机器172.29.230.6，同城主机器2条数据可正常同步到同城备机器，并且观察2台机器数据不重复；
image7.png

增量同步验证
1）通过造数demo往同城主机器172.30.94.25写入1条数据
image8.png

2）logstash执行任务时，同步数据到同城备机器172.29.230.6，2台机器数据一致；后续logstash定时任务一直在执行，根据callid唯一标识，若主机器无数据新增，备机器则只做更新。
image9.png


2.3.7.2、单索引 同城备-->异地和同城主
配置和启动logstash，并设置每分钟同步一次；
image10.png

同城备机器无数据新增
1）同城主机器172.30.94.25无数据写入
image11.png

2）异地机器172.29.67.75写入3条数据，数据写入正常
image12.png

同城备机器新增数据
1）通过造数demo往elasticsearch同城备机器172.29.230.6写入1条数据
image13.png

2）新增的1条数据，同步到同城主机器172.30.94.25成功
image14.png

3）新增的1条数据，同步到异地机器172.29.67.75成功
image15.png


2.3.7.3、单索引 异地-->同城主和同城备
配置和启动logstash，并设置每分钟同步一次；
image16.png

1）使用造数demo往异地机器172.29.67.75，写入1条数据；
image17.png

2）观察同城主备机器，该数据同步正常
同城主
image18.png

同城备
image19.png


2.3.7.4、双索引    同城主-->同城备
配置和启动logstash，并设置每分钟同步一次；     注意写判断逻辑！！！
image20.png

1）通过造数demo往elasticsearch同城主机器172.30.94.25的srp_operate_dialog、srp_knowledge_publish索引各写入21条数据；
image21.png

2）同城备机器172,29.230.6，写入数据正常
image22.png


2.3.8、数据校验（同步结束和数据一致性）
编写shell脚本对比索引count，count一致则表示数据同步完成
[indexCount.sh]
执行 sh indexCount.sh
image23.png


java编码实现，对比count和数据ID


Plain Text
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.apache.http.HttpHost;
import org.elasticsearch.client.RestClient;
import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.ElasticsearchException;
import co.elastic.clients.elasticsearch.core.SearchRequest;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import co.elastic.clients.elasticsearch.core.search.Hit;
import co.elastic.clients.json.jackson.JacksonJsonpMapper;
import co.elastic.clients.transport.ElasticsearchTransport;
import co.elastic.clients.transport.rest_client.RestClientTransport;

public class EeasticDataTest2 {

        private static final String INDEX_NAME = "srp_operate_dialog";
        private static final String host1 = "172.30.94.25";
        private static final String host2 = "172.29.230.6";
        private static final String host3 = "172.29.67.75";

        public static void main(String[] args) throws ElasticsearchException, IOException {                

                // 同城主机器
                RestClient restClient1 = RestClient.builder(new HttpHost(host1, 9200, "http")).build();
                ElasticsearchTransport transport1 = new RestClientTransport(restClient1, new JacksonJsonpMapper());
                ElasticsearchClient client1 = new ElasticsearchClient(transport1);

                // 同城备机器
                RestClient restClient2 = RestClient.builder(new HttpHost(host2, 9200, "http")).build();
                ElasticsearchTransport transport2 = new RestClientTransport(restClient2, new JacksonJsonpMapper());
                ElasticsearchClient client2 = new ElasticsearchClient(transport2);
                
                // 异地机器
                RestClient restClient3 = RestClient.builder(new HttpHost(host3, 9200, "http")).build();
                ElasticsearchTransport transport3 = new RestClientTransport(restClient3, new JacksonJsonpMapper());
                ElasticsearchClient client3 = new ElasticsearchClient(transport3);

                int docCount1 = getDocumentCount(client1, INDEX_NAME);
                System.out.println( host1 + "的索引" + INDEX_NAME + "总数为: " + docCount1);

                int docCount2 = getDocumentCount(client2, INDEX_NAME);
                System.out.println( host2 + "的索引" + INDEX_NAME + "总数为: " + docCount2);
                
                int docCount3 = getDocumentCount(client3, INDEX_NAME);
                System.out.println( host3 + "的索引" + INDEX_NAME + "总数为: " + docCount3);

                // 比较文档总数
                if ( docCount1 == docCount2 && docCount2 == docCount3 ) {
                        System.out.println("文档总数一致");
                } else {
                        System.out.println("文档总数不一致");
                        // 文档总数不一致，无需比较
                        // return;
                }

                // 如果文档总数一致，查看数据是否一致
                compareDocuments(client1, client2, client3, INDEX_NAME);
        }

        private static int getDocumentCount(ElasticsearchClient client, String indexName) throws IOException {
                SearchRequest searchRequest = SearchRequest.of(s -> s
                                .index(indexName)
                                .size(0) 
                );
                SearchResponse<Object> response = client.search(searchRequest, Object.class);
                return (int) response.hits().total().value();
        }

        private static void compareDocuments(ElasticsearchClient client1, ElasticsearchClient client2, ElasticsearchClient client3, String indexName)
                        throws IOException {
                SearchRequest searchRequest = SearchRequest.of(s -> s
                                .index(indexName)
                                // 不能超过ES配置数量，否则会报错
                                .size(1000) 
                );
                SearchResponse<Map> response1 = client1.search(searchRequest, Map.class);
                SearchResponse<Map> response2 = client2.search(searchRequest, Map.class);
                SearchResponse<Map> response3 = client3.search(searchRequest, Map.class);

                Map<String, Map> docs1 = new HashMap<>();
                for (Hit<Map> hit : response1.hits().hits()) {
                        docs1.put(hit.id(), hit.source());
                }

                Map<String, Map> docs2 = new HashMap<>();
                for (Hit<Map> hit : response2.hits().hits()) {
                        docs2.put(hit.id(), hit.source());
                }
                
                Map<String, Map> docs3 = new HashMap<>();
                for (Hit<Map> hit : response3.hits().hits()) {
                        docs3.put(hit.id(), hit.source());
                }

                for (String id : docs1.keySet()) {
                        
                        if (!docs2.containsKey(id)) {
                                System.out.println(host2 + "的" + INDEX_NAME + "索引缺失ID为: " + id );
                        }
                        
                        if (!docs3.containsKey(id)) {
                                System.out.println(host3 + "的" + INDEX_NAME + "索引缺失ID为: " + id );
                        }
                        
                }
        }
}
执行验证结果：
image24.png


2.3.9、数据同步效率验证
2.3.9.1、全量641006条数据
logstash未运行，往同城主写入641006条数据
image25.png

数据同步
启动logstash，开启数据同步。耗时1分25秒
开始同步时间为：15:24:43
结束同步时间为：15:26:08

2.3.9.2、全量5198000条数据
logstash未运行，往同城主写入5198000条数据
image26.png

数据同步
启动logstash，开启数据同步。耗时9分52秒


Plain Text
POST /srp_operate_dialog/_search

{
  "size": 0,
  "aggs": {
    "min_timestamp": {
      "min": {
        "field": "@timestamp"
      }
    },
    "max_timestamp": {
      "max": {
        "field": "@timestamp"
      }
    }
  }
}
image27.png

2.3.9.3、增量1000路并发
同城主、同城备中已有5747505条数据，logstash启动状态
通过造数demo按照1000路并发往同城主写入数据。同时，同城主将1000条数据同步到同城备，数据同步完成，耗时9分47秒。
原因分析：未设置同步当前小时或者当天的数据，全量同步更新数据，导致时间长。
image28.png

image29.png

2.4、minio两地三中心方案
 @罗海红 hhluo
2.4.1、部署架构
image30.png

部署方案说明：
1.同城采用mc replicate桶复制进行单向同步，桶复制会同时同步对象的版本历史和元数据，覆盖目标集群，保障目标集群和源集群对象数据、版本历史和元数据的一致，且无时延。
2.异地采用mc mirror镜像同步进行单向同步，镜像仅同步当前对象，不包含任何版本信息或元数据，删除文件也不会被同步到目标集群，因此会存在源集群和目标集群不一致的情况，需要进行定时比对并进行补偿处理。
注意事项：
mc relicate默认只同步设之后的变化，如果要同步之前的变化，需要设置时间，或者全量同步一次。
mc relicate没有独立的同步进程，也没有同步日志，且一个节点只能设置一个同步配置，且只能在触发服务器上生效，mc relicate的目标服务器上不生效。
mc mirror有独立的同步进程，有同步日志，不同步删除动作，需要通过定时任务处理删除动作。
一个节点可以启动多个mc mirror监控进程。
2.4.2、部署规划
部署三个中心minio


组件

版本

下载地址

minio

RELEASE.2023-06-29T05-12-28Z

https://dl.min.io/server/minio/release/linux-amd64/minio.RELEASE.2023-06-29T05-12-28Z

mc

RELEASE.2024-11-21T17-21-54Z

https://dl.min.io/client/mc/release/linux-amd64/mc.RELEASE.2024-11-21T17-21-54Z
主机安装规划说明


机房

host

minio

mc

数据校验补偿脚本

主城主(合肥城市云机房)

172.29.230.6

安装

1.安装
2.配置桶复制
3.启动镜像同步进程

启动定时任务

同城备(合肥B3-5机房)

172.30.94.25

安装

安装



异地(上海临港机房)

172.29.67.75

安装

安装


1）同城主：
minio：http://172.29.230.6:9100/            部署用户名/密码：minioadmin/minioadmin
2）同城备：
minio：http://172.30.94.25:9100/            部署用户名/密码：minioadmin/minioadmin
3）异地：
minio：http://172.29.67.75:9100/           部署用户名/密码：minioadmin/minioadmin
2.4.3、部署操作
部署验证步骤：
环境如下：
合肥城市云机房minio节点：172.29.230.6
合肥B3-5机房minio节点：172.30.94.25
上海临港机房minio节点：172.29.67.75
所有节点下载minio.RELEASE.2023-06-29T05-12-28Z（注：配置存储桶复制的前提是 目标minio与源minio必须版本相同）


Bash
mkdir -p /data/minio/{bin,data,logs}
curl -L https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.2023-06-29T05-12-28Z -o /data/minio/bin/minio
curl -sL https://dl.min.io/client/mc/release/linux-amd64/mc.RELEASE.2024-11-17T19-35-25Z -o /data/minio/bin/mc
chmod +x /data/minio/bin/minio
chmod +x /data/minio/bin/mc
所有节点：编写/data/minio/bin/start.sh脚本，启动minio进程


Bash
#!/bin/sh
export MINIO_ROOT_USER=minioadmin
export MINIO_ROOT_PASSWORD=minioadmin
/data/minio/bin/minio server /data/minio/data --address=":9000" --console-address=":9100"  /data/minio/logs/minio.log 2>&1 &
所有节点：针对3个minio节点，创建3个别名


Bash
/data/minio/bin/mc alias set minio1 http://172.29.230.6:9000 minioadmin minioadmin --api s3v4
/data/minio/bin/mc alias set minio2 http://172.30.94.25:9000 minioadmin minioadmin --api s3v4
/data/minio/bin/mc alias set minio3 http://172.29.67.75:9000 minioadmin minioadmin --api s3v4
/data/minio/bin/mc alias list
172.29.230.6节点：针对3个minio节点，各自创建1个bucket并启用版本控制


Bash
/data/minio/bin/mc mb -p minio1/bucket111
/data/minio/bin/mc mb -p minio2/bucket111
/data/minio/bin/mc mb -p minio3/bucket111
/data/minio/bin/mc version enable minio1/bucket111
/data/minio/bin/mc version enable minio2/bucket111
/data/minio/bin/mc version enable minio3/bucket111
172.29.230.6节点：配置单向同步的主备复制，实现同城备份机房和异地备份机房的同步


Bash
/data/minio/bin/mc replicate add --remote-bucket http://minioadmin:minioadmin@172.30.94.25:9000/bucket111 --replicate "delete,delete-marker,existing-objects" --sync minio1/bucket111 --limit-upload 200Mi
13.172.29.230.6节点：配置mirror备份监控


Bash
/data/minio/bin/mc config host add minio1 http://172.29.230.6:9000 minioadmin minioadmin
/data/minio/bin/mc config host add minio2 http://172.30.94.25:9000 minioadmin minioadmin
/data/minio/bin/mc config host add minio2 http://172.30.94.25:9000 minioadmin minioadmin
/data/minio/bin/mc mirror --remove --overwrite --watch minio1 minio3 > /dev/null 2>&1 &
14.启动mirror镜像同步进程


Bash
/data/minio/bin/mc mirror --remove --overwrite --watch minio1 minio3
2.4.4、数据校验补偿
对比补充操作说明：
可以采用minio自带的mc diff判别不同集群桶一致性的工具
image31.png

比对补偿脚本如下：


Bash
#!/bin/bash
 
MININ_ROOT_PATH=/data/minio/bin
 
# 主节点minio
MASTER_MINIO="http://172.29.230.6:9000/"
# 源集群(主节点)别称
MASTER_ALIAS="minio1"  
# 异地备份节点minio
REMOTE_BACK_MINIO="http://172.29.67.75:9000/"
# 目标集群(异地备份节点)别称
REMOTE_BACK_ALIAS="minio3"  
# 桶名
BUCKET="bucket111" 
 
TMP_PATH=/data/minio
 
${MININ_ROOT_PATH}/mc diff ${MASTER_ALIAS}/${BUCKET} ${REMOTE_BACK_ALIAS}/${BUCKET} > ${TMP_PATH}/diff.log
 
DIFF_COUNT=`cat ${TMP_PATH}/diff.log | wc -l`
 
if [$DIFF_COUNT -eq 0] 
    then
        echo "主节点和异地备份节点文件一致."
    else
        echo "主节点和异地备份节点文件不一致."
        for line in $(cat ${TMP_PATH}/diff.log ${TMP_PATH}/diff.log | grep '<' | awk -F '< ' '{print $2}'| sed 's/${MASTER_MINIO}//g')
        do
            ${MININ_ROOT_PATH}/mc cp ${MASTER_ALIAS}/${BUCKET}/${line} ${REMOTE_BACK_ALIAS}/${BUCKET}/${line}
        done
            
        for line in $(cat ${TMP_PATH}/diff.log ${TMP_PATH}/diff.log | grep '>' | awk -F '> ' '{print $2}'| sed 's/${REMOTE_BACK_MINIO}//g')
        do
            ${MININ_ROOT_PATH}/mc rm ${REMOTE_BACK_ALIAS}/${BUCKET}/${line}
        done
            
        for line in $(cat ${TMP_PATH}/diff.log ${TMP_PATH}/diff.log | grep '!' | awk -F '! ' '{print $2}'| sed 's/${MASTER_MINIO}//g')
        do
            ${MININ_ROOT_PATH}/mc rm ${REMOTE_BACK_ALIAS}/${BUCKET}/${line}
            ${MININ_ROOT_PATH}/mc cp ${MASTER_ALIAS}/${BUCKET}/${line} ${REMOTE_BACK_ALIAS}/${BUCKET}/${line}
        done
fi
2.4.5、主从切换操作
2.4.5.1、手动切换步骤
2.4.6、注意事项及问题

2.5、redis两地三中心方案
2.5.1、部署架构
image32.png


方案说明：
1. **每个中心内部**：部署Redis Cluster集群（3主3从），实现本地数据分片与高可用，主节点负责读写，从节点提供读扩展和故障转移。
2. **跨中心同步**：通过Redis-Shake双向数据同步工具实现跨集群数据同步，同步策略如下：
   - **同城主备中心（A↔B）**：**同步复制**（强一致性优先），利用低延迟专线保障数据实时性。
   - **异地灾备中心（C）**：**异步复制**（最终一致性优先），容忍网络延迟但保障异地容灾能力。
3. **全局流量调度**：通过全局负载均衡（nginx）和健康检查机制，动态路由用户请求至可用中心，主中心故障时自动切换至同城或异地灾备中心。

2.5.2、部署规划
部署三个中心redis集群
1）同城主：
Redis ClusterA：172.30.94.25  7001、7002 、7003、7004、7005、7006     
2）同城备：
Redis ClusterB：172.29.230.6  7001、7002 、7003、7004、7005、7006 
3）异地：
Redis ClusterC：172.29.67.75  7001、7002  、7003、7004、7005、7006 

2.5.3、部署操作
1.安装与集群配置：
#所有节点安装Redis
sudo apt-get install redis-server

2.创建集群（三主三从）
Redis ClusterA:
redis-cli --cluster create 172.30.94.25:7000 172.30.94.25:7001 172.30.94.25:7002 172.30.94.25:7003 172.30.94.25:7004 172.30.94.25:7005 172.30.94.25:7006
--cluster-replicas 1 -a yourpassword
Redis ClusterB:
redis-cli --cluster create 172.29.230.6:7000 172.29.230.6:7001 172.29.230.6:7002 172.29.230.6:7003 172.29.230.6:7004 172.29.230.6:7005 172.29.230.6:7006
--cluster-replicas 1 -a yourpassword
Redis ClusterC:
redis-cli --cluster create 172.29.67.75:7000 172.29.67.75:7001 172.29.67.75:7002 172.29.67.75:7003 172.29.67.75:7004 172.29.67.75:7005 172.29.67.75:7006
--cluster-replicas 1 -a yourpassword

3.配置跨中心同步
   # 使用Redis-Shake同步A中心到B中心
   ./redis-shake -type sync -source "redis://172.30.94.25:7000" -target "redis://172.29.230.6:7000"

4.部署全局调度组件
Nginx配置（GSLB）
   upstream redis_cluster {
     server 172.30.94.25:7000 weight=5;  # 主中心
     server 172.29.230.6:7000 backup;    # 同城灾备
     server 172.29.67.75:7000 backup;       # 异地灾备
   }

2.5.4、数据校验补偿
1.校验工具
使用redis-check-aof修复AOF文件。
对比主从节点的master_repl_offset，监控同步延迟

2.数据补偿
故障后通过业务日志重放丢失的写操作
定期备份RDB/AOF至异地中心，结合BGSAVE实现冷备恢复

2.5.5、主从切换操作
1.手动切换
#平时不启动，灾难时手动激活，数据恢复后通过CLUSTER FAILOVER接管
#提升从节点为主
SLAVEOF NO ONE
#更新其他节点指向新主
SLAVEOF <新主IP> 7000

2.切换后验证
验证INFO replication中角色变更。
客户端重连配置更新
2.5.6、注意事项及问题
1.一致性问题：
异步复制可能导致异地灾备数据延迟（RPO>0），强一致性场景需半同步复制。
避免跨机房事务操作，改用本地事务+最终一致性。
2.网络风险：
同城专线故障可能引发脑裂，需配置min-slaves-to-write限制写入条件。
监控CLUSTER INFO中的cluster_state和cluster_slots_assigned。
3.运维禁忌：
禁止生产环境使用FLUSHALL、KEYS *等高危命令。
避免大Key（如Value>10KB），拆分Hash/List结构。
2.6、mongodb两地三中心方案
2.6.1、部署架构
image33.png

方案说明：
1.方案主要利用副本集部署中，MongoDB 在主节点上应用写入操作，然后在主节点的 oplog 上记录这些操作。从节点成员复制该日志，并将操作应用于其数据集的特性。
image34.png

2.使用开源mongo同步组件MongoShake，MongoShake是阿里云以Golang语言编写的通用平台型服务工具，它通过读取MongoDB的Oplog操作日志来复制MongoDB的数据以实现特定需求。
2.6.2、部署规划
部署三个中心mongodb


组件

版本

下载地址

mongodb

4.4.29

https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.29.tgz  

mongoshake

2.8.4

https://github.com/alibaba/MongoShake/releases/download/release-v2.8.4-20230425/mongo-shake-v2.8.4.tgz 
主机安装规划说明


机房

host

mongodb

mongoshake

数据校验补偿脚本

主城主(合肥城市云机房)

172.29.230.6

安装1主1从1仲裁

1.安装不启动

启动定时任务

同城备(合肥B3-5机房)

172.30.94.25

安装1主1从1仲裁

1.安装
2.配置启动



异地(上海临港机房)

172.29.67.75

安装单机

1.安装
2.配置启动


1）同城主：
mongodb：
主: 172.29.230.6:27017
从: 172.29.230.6:27018
仲裁: 172.29.230.6:27019
2）同城备：
主: 172.29.230.6:27017
从: 172.29.230.6:27018
仲裁: 172.29.230.6:27019
3）异地：
172.29.230.6:27017
2.6.3、部署操作
2.6.3.1、下载解压安装包


Bash
wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.29.tgz
tar xzvf mongodb-linux-x86_64-rhel70-4.4.29.tgz -C /data/mongo/mongodb-4.4.29
2.6.3.2、创建主节点
2.6.3.2.1、创建存储数据和日志的目录


Bash
mkdir -p /data/mongo/replica_sets/mongo_27017/log
mkdir -p /data/mongo/replica_sets/mongo_27017/data
2.6.3.2.2、新建配置文件


Bash
vim /data/mongo/replica_sets/mongo_27017/mongod.conf

systemLog:                                                                                                                                                                                                     
    destination: file  
    path: "/data/mongo/replica_sets/mongo_27017/log/mongod.log"                                                                                                                                                
    logAppend: true                                                                                                                                                                                            
storage:                                                                                                                                                                                                       
    dbPath: "/data/mongo/replica_sets/mongo_27017/data"                                                                                                                                                        
    journal:                                                                                                                                                                                                   
    # 启 用 持 久 性 日 志                                                                                                                                                                                            
        enabled: true                                                                                                                                                                                          
processManagement:                                                                                                                                                                                             
    fork: true                                                                                                                                                                                                 
    pidFilePath: "/data/mongo/replica_sets/mongo_27017/log/mongod.pid"                                                                                                                                         
net:                                                                                                                                                                                                           
    bindIp: localhost,172.29.230.6                                                                                                                                                                             
    port: 27017                                                                                                                                                                                                
replication:                                                                                                                                                                                                   
    replSetName: mongors 
2.6.3.2.3、启动主节点服务


Bash
/data/mongo/mongodb-4.4.29/bin/mongod -f /data/mongo/replica_sets/mongo_27017/mongod.conf
about to fork child process, waiting until server is ready for connections.
forked process: 37112
child process started successfully, parent exiting
2.6.3.3、创建副本节点
2.6.3.3.1、创建存储数据和日志的目录


Bash
mkdir -p /data/mongo/replica_sets/mongo_27018/log
mkdir -p /data/mongo/replica_sets/mongo_27018/data
2.6.3.3.2、新建配置文件


Bash
vim /data/mongo/replica_sets/mongo_27018/mongod.conf

systemLog:                                                                                                                                                                                                     
    destination: file                                                                                                                                                                                          
    path: "/data/mongo/replica_sets/mongo_27018/log/mongod.log"                                                                                                                                                
    logAppend: true                                                                                                                                                                                            
storage:                                                                                                                                                                                                       
    dbPath: "/data/mongo/replica_sets/mongo_27018/data"                                                                                                                                                        
    journal:                                                                                                                                                                                                   
    # 启 用 持 久 性 日 志                                                                                                                                                                                            
        enabled: true                                                                                                                                                                                          
processManagement:                                                                                                                                                                                             
    fork: true                                                                                                                                                                                                 
    pidFilePath: "/data/mongo/replica_sets/mongo_27018/log/mongod.pid"                                                                                                                                         
net:                                                                                                                                                                                                           
    bindIp: localhost,172.29.230.6                                                                                                                                                                             
    port: 27018                                                                                                                                                                                                
replication:                                                                                                                                                                                                   
    replSetName: mongors
2.6.3.3.3、启动节点服务


Bash
/data/mongo/mongodb-4.4.29/bin/mongod -f /data/mongo/replica_sets/mongo_27018/mongod.conf
about to fork child process, waiting until server is ready for connections.
forked process: 38263
child process started successfully, parent exiting
2.6.3.4、创建仲裁节点
2.6.3.4.1、创建存储数据和日志的目录


Bash
mkdir -p /data/mongo/replica_sets/mongo_27019/log
mkdir -p /data/mongo/replica_sets/mongo_27019/data
2.6.3.4.2、创建配置文件 


Bash
vim /data/mongo/replica_sets/mongo_27019/mongod.conf

systemLog:                                                                                                                                                                                                     
    destination: file                                                                                                                                                                                          
    path: "/data/mongo/replica_sets/mongo_27019/log/mongod.log"                                                                                                                                                
    logAppend: true                                                                                                                                                                                            
storage:                                                                                                                                                                                                       
    dbPath: "/data/mongo/replica_sets/mongo_27019/data"                                                                                                                                                        
    journal:                                                                                                                                                                                                   
    # 启 用 持 久 性 日 志                                                                                                                                                                                            
        enabled: true                                                                                                                                                                                          
processManagement:                                                                                                                                                                                             
    fork: true                                                                                                                                                                                                 
    pidFilePath: "/data/mongo/replica_sets/mongo_27019/log/mongod.pid"                                                                                                                                         
net:                                                                                                                                                                                                           
    bindIp: localhost,172.29.230.6                                                                                                                                                                             
    port: 27019                                                                                                                                                                                                
replication:                                                                                                                                                                                                   
    replSetName: mongors
2.6.3.4.3、启动节点服务


Bash
/data/mongo/mongodb-4.4.29/bin/mongod -f /data/mongo/replica_sets/mongo_27019/mongod.conf
about to fork child process, waiting until server is ready for connections.
forked process: 38314
child process started successfully, parent exiting
2.6.3.5、添加环境变量


Bash
vim /etc/profile
 
export PATH=/data/mongo/mongodb-4.4.29/bin/:$PATH
 
source /etc/profile
2.6.3.6、初始化副本集
2.6.3.6.1、客户端连接主节点


Bash
mongo --host 172.29.230.6 --port=27017
2.6.3.6.2、初始化副本集
当前并未选举主节点（primary），并且操作没有设置为允许从辅助节点（secondary）上读取。所以很多命令无法使用，必须初始化副本集


Bash
rs.initiate()
{
        "info2" : "no configuration specified. Using a default configuration for the set",
        "me" : "172.29.230.6:27017",
        "ok" : 1
}
mongors:SECONDARY>
mongors:PRIMARY>
mongors:PRIMARY>
mongors:PRIMARY>
"ok"的值为1，说明创建成功。 命令行提示符发生变化，变成了一个从节点角色，此时默认不能读写。稍等片刻，发现副本集只有自己一个，变成主节点。
2.6.3.7、查看副本集信息
2.6.3.7.1、查看副本集的配置内容


Bash
mongors:PRIMARY> rs.config()
{
        "_id" : "mongors",
        "version" : 1,
        "term" : 1,
        "protocolVersion" : NumberLong(1),
        "writeConcernMajorityJournalDefault" : true,
        "members" : [
                {
                        "_id" : 0,
                        "host" : "172.29.230.6:27017",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                }
        ],
        "settings" : {
                "chainingAllowed" : true,
                "heartbeatIntervalMillis" : 2000,
                "heartbeatTimeoutSecs" : 10,
                "electionTimeoutMillis" : 10000,
                "catchUpTimeoutMillis" : -1,
                "catchUpTakeoverDelayMillis" : 30000,
                "getLastErrorModes" : {

                },
                "getLastErrorDefaults" : {
                        "w" : 1,
                        "wtimeout" : 0
                },
                "replicaSetId" : ObjectId("662fa68cd74e84faba9867e0")
        }
}
提示：
rs.config()是该方法的别名。
configuration：可选，如果没有配置，则使用默认主节点配置。
说明：
"_id" : "mongors" ：副本集的配置数据存储的主键值，默认就是副本集的名字
"members" ：副本集成员数组，此时只有一个："host" : "172.29.230.6:27017"
该成员不是仲裁节点： "arbiterOnly" : false
优先级（权重值）： "priority" : 1
"settings" ：副本集的参数配置。
2.6.3.7.2、查看集群状态


Bash
rs.status()
2.6.3.8、添加副本节点和仲裁节点
2.6.3.8.1、添加副本节点


Bash
mongors:PRIMARY> rs.add("172.29.230.6:27018")
{
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1714399646, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1714399646, 1)
}
2.6.3.8.2、添加仲裁节点


Bash
mongors:PRIMARY> rs.addArb("172.29.230.6:27019")
{
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1714399738, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1714399738, 1)
}
2.6.3.9、再次查看副本集配置信息


Bash
mongors:PRIMARY> rs.conf()
{
        "_id" : "mongors",
        "version" : 3,
        "term" : 1,
        "protocolVersion" : NumberLong(1),
        "writeConcernMajorityJournalDefault" : true,
        "members" : [
                {
                        "_id" : 0,
                        "host" : "172.29.230.6:27017",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                },
                {
                        "_id" : 1,
                        "host" : "172.29.230.6:27018",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                },
                {
                        "_id" : 2,
                        "host" : "172.29.230.6:27019",
                        "arbiterOnly" : true,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 0,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                }
        ],
        "settings" : {
                "chainingAllowed" : true,
                "heartbeatIntervalMillis" : 2000,
                "heartbeatTimeoutSecs" : 10,
                "electionTimeoutMillis" : 10000,
                "catchUpTimeoutMillis" : -1,
                "catchUpTakeoverDelayMillis" : 30000,
                "getLastErrorModes" : {

                },
                "getLastErrorDefaults" : {
                        "w" : 1,
                        "wtimeout" : 0
                },
                "replicaSetId" : ObjectId("662fa69cd74e84faba8865e0")
        }
}
成员信息：
成员1（_id: 0）和成员2（_id: 1）都是数据承载节点，它们位于同一台机器的不同端口（27017 和 27018）。两者都有相同的优先级（"priority" : 1），意味着它们都可以成为主节点。它们都没有延迟复制（"slaveDelay" : NumberLong(0)），并且都参与选举投票（"votes" : 1）。
成员3（_id: 2）是一个仲裁者（arbiterOnly: true），位于端口27019。仲裁者的角色是在选举新主节点时起到决定性的一票作用，但它不存储实际数据，也没有优先级，不参与数据复制。
2.6.3.10、设置SECONDARY副本节点可读


Bash
rs.secondaryOk()
2.6.3.11、配置mongoshake
2.6.3.11.1、修改collector.conf配置文件


Bash
# 源 MongoDB连接串信息，逗号分隔同一个副本集内的结点，分号分隔分片sharding实例，免密模式                                                                                                                 
# 可以忽略 “username:password@”，注意，密码里面不能含有'@'符号 。                                                                                                                                         
# 举例 ：                                                                                                                                                                                                
# 副本集 ： mongodb://username1:password1@primaryA,secondaryB,secondaryC                                                                                                                                 
# 分片集 ： mongodb://username1:password1@primaryA,secondaryB,secondaryC;mongodb://username2:password2@primaryX,secondaryY,secondaryZ                                                                    
mongo_urls = mongodb://172.29.230.6:27017,172.29.230.6:27018,172.29.230.6:27019

#同步模式，all表示全量+增量同步，full表示全量同步，incr表示增量同步
sync_mode = all
 
# 通道模式。
tunnel = direct

mongo_connect_mode = secondaryPreferred 
 
filter.ddl_enable = true
filter.oplog.gids = false
 
# checkpoint的具体写入的MongoDB地址，用于支持断点续传。如果不配置，对于副本集和分片集群都将写入源库(db=mongoshake)
checkpoint.storage.url =
checkpoint.storage.db = mongoshake
checkpoint.storage.collection = ckpt_default
 
# 全量期间数据同步完毕后，是否需要创建索引，none表示不创建，foreground表示创建前台索引 ，background表示创建后台索引。 
full_sync.create_index = background

incr_sync.mongo_fetch_method = oplog
2.6.3.11.2、启动mongoshake并打印日志信息


Bash
./collector.linux -conf=collector.conf -verbose 1
会发现源端自动创建了一个mongoshake库：


Bash
mongors:PRIMARY> show dbs                                                                                                                                                                                      
admin       0.000GB                                                                                                                                                                                            
config      0.000GB                                                                                                                                                                                            
local       0.001GB                                                                                                                                                                                            
mongoshake  0.000GB                                                                                                                                                                                            
test1       0.000GB                                                                                                                                                                                            
mongors:PRIMARY> use mongoshake;                                                                                                                                                                               
switched to db mongoshake                                                                                                                                                                                      
mongors:PRIMARY> show tables;                                                                                                                                                                                  
ckpt_default
2.6.3.11.3、监控MongoShake状态


Bash
./mongoshake-stat --port=9100
字段说明： 
logs_get：一秒钟内我们获得了多少个oplog。
logs_repl：我们在一秒钟内重播了多少oplog。
logs_success：我们在一秒钟内成功重播了多少操作日志，即TPS。
2.6.4、数据校验补偿
在MongoShake 中藏着这样一个脚本comparison.py，这是一个迁移完成后做全量比对的一个工具。
对比补充操作说明：


Bash
python3 ./comparison.py --src="52.83.40.71:27017" --dest="127.0.0.1:27017" --count=10000 --excludeDbs=admin --excludeCollections=system.profile --comparisonMode=sample

#--comparisonMode=no 表示使用统计信息比对记录数
#--comparisonMode=sample 表示只统计采样部分数据看是否一致,采样的数据由--count参数控制
#--comparisonMode=all 表示分批次对比所有的文档是否一致(非常慢)
对比完成如下图：
image35.png

2.6.5、主从切换操作
2.6.5.1、手动切换步骤
2.6.6、注意事项及问题
2.6.6.1、权限
  对于完全同步，MongoShake需要每个数据库的读取权限。对于增量，MongoShake需要local数据库的读取权限和数据库的写入权限mongoshake。
2.6.6.2、MongoShake重启
  sync.mode是all，重启后如果检查点存在且有效，这意味着最早的操作日志小于检查点，则MongoShake将仅运行增加同步。否则，MongoShake将再次运行完全同步，此后，将运行增加同步。因此，如果用户仍想运行完全同步但检查点存在，mongoshake.ckpt_default则应手动删除检查点。
2.6.6.3、MongoShake支持同步DDL
  使用replayer.dml_only选项。但是DDL不是幂等操作，一旦失败，oplog可能会重放，因此启用DDL在最新版本中可能会出现问题。
2.6.6.4、断点恢复
  MongoShake支持基于检查点机制的断点恢复，每次启动时，它都会读取检查点，这是一个时间戳，指示已准备好重播多少数据。之后，它将从此时间戳开始从源中提取数据。因此重启时不会丢失数据。
2.6.6.5、oplog一致性
mongoshake不支持oplog的严格一致性，当shard_key是auto/collection，mongoshake支持顺序一致性其中同一命名空间（装置ns），该序列可以得到保证。如果shard_key为id，则mongoshake支持最终一致性。

3、两地三中心测试方案
3.1、测试准备
3.1.1、测试环境准备
以下中心A和中心B是同城机房，中心C是异地机房。机房间专线网络，带宽XX，延时XX（不同数据规模和并发，对网络带宽的要求）


机房

详情

中心 A

对照高可用部署文档，搭建 MySQL集群
对照高可用部署文档，搭建 Redis 集群
对照高可用部署文档，搭建 ES 集群
对照高可用部署文档，搭建 MinIO 集群
对照高可用部署文档，搭建 MongoDB 集群
对照高可用部署文档，搭建 zookeeper集群
对照高可用部署文档，搭建 doris 集群
对照高可用部署文档，搭建 nacos集群，连接本中心的mysql
搭建监控告警平台，如星际可观测平台
对照高可用部署文档，搭建服务机器人产品平台应用组件和依赖的CBB组件（uap、docqa、知识理解引擎、大模型能力平台、数据能力平台、大模型），连接上述的中间件，服务机器人对接星际可观测

中心 B

对照高可用部署文档，搭建 MySQL集群
对照高可用部署文档，搭建 Redis 集群
对照高可用部署文档，搭建 ES 集群
对照高可用部署文档，搭建 MinIO 集群
对照高可用部署文档，搭建 MongoDB 集群
对照高可用部署文档，搭建 zookeeper集群
对照高可用部署文档，搭建 doris 集群
对照高可用部署文档，搭建 nacos集群，连接本中心的mysql
搭建监控告警平台，如星际可观测平台
对照高可用部署文档，搭建服务机器人产品平台应用组件和依赖的CBB组件（uap、docqa、知识理解引擎、大模型能力平台、数据能力平台、大模型），连接中心A的mysql、es、minio、mongodb、zookeeper、doris，连接本中心的redis和nacos，对接本中心的星际可观测

中心 C

对照高可用部署文档，搭建 MySQL集群
对照高可用部署文档，搭建 Redis 集群
对照高可用部署文档，搭建 ES 集群
对照高可用部署文档，搭建 MinIO 集群
对照高可用部署文档，搭建 MongoDB 集群
对照高可用部署文档，搭建 zookeeper集群
对照高可用部署文档，搭建 doris 集群
对照高可用部署文档，搭建 nacos集群，连接本中心的mysql
搭建监控告警平台，如星际可观测平台
对照高可用部署文档，搭建服务机器人产品平台应用组件和依赖的CBB组件（uap、docqa、知识理解引擎、大模型能力平台、数据能力平台、大模型），连接本中心的mysql、redis、es、minio、mongodb、zookeeper、doris、nacos，对接本中心的星际可观测


3.1.2、测试数据准备


机房

详情

中心 A

可复用性能测试数据，数据量级如下：
准备 5 万es知识片段数据（立项指标）
准备 100 mysql和redis的意图数据（立项指标）
准备18万 mysql搜索记录数据（一天搜索1000次，半年18万）
准备1800万es对话记录数据（一天10万，半年1800万）
准备大小3G、数量1.5万的 minio数据（主要是素材知识文件）
准备mongodb数据（主要是docqa存储的数据，满足5万知识片段和100意图）
准备日二十万通话、离线1年、明细半年的 报表doris数据
运行性能测试脚本1小时，使redis、zookeeper中存在运行后的数据

中心B

完成表结构初始化，数据均为空

中心C

完成表结构初始化，数据均为空


3.1.3、测试工具准备


工具名称

工具说明

性能测试工具

选型: 自研python/jmeter脚本
功能: 支持知识问答、知识搜索和对话控制接口并发测试。运行脚本用于产生系统初始数据和增量数据

Mysql数据一致性对比工具

选型: mysqldiff
功能: 对比中心A和中心B、中心C之间mysql集群数据的一致性

Minio数据一致性对比工具

选型: mc
功能: 对比中心A和中心B、中心C之间minio集群数据的一致性

Es数据一致性对比工具

选型: 自研脚本
功能: 对比中心A和中心B、中心C之间es集群数据的一致性

Redis数据一致性对比工具

选型: 自研脚本
功能: 对比中心A和中心B、中心C之间redis集群内离线配置数据的一致性

Mongodb数据一致性对比工具

选型: 自研脚本
功能: 对比中心A和中心B、中心C之间mongodb集群数据的一致性

系统监控预警工具

选型: 星际可观测平台
功能: 业务探活、组件自拉起、机器监控、邮件预警

混沌测试工具

选型: chaosblade
功能: 中心A、中心B、中心C上网络延迟、网络抖动等异常场景模拟

网络测试工具

选型: iperf sockperf
功能: 中心A和中心B、中心C之间网络带宽和延迟测试

3.2、测试内容
3.2.1、测试内容概览
为模拟峰值业务压力下的数据同步、故障切换、数据恢复等功能，以下场景均在系统性能测试中的前提下开展。性能测试完成后统计数据一致性


测试项

说明

系统可用性测试

验证两地三中心部署架构下，无灾难发生时的系统可用性

数据同步测试

测试场景：
初始全量数据同步测试
持续增量数据同步测试

关注点：
数据一致性和完整性
数据同步效率

故障切换测试

测试场景：
单节点故障，即单中心高可用验证
单个公共组件灾备切换（mysql、minio、es、mongodb、doris、zookeeper、redis）
同城灾备切换（中心A故障）
同城灾备切换（中心B故障）
异地灾备切换（中心A B均故障）
异地中心故障（中心C故障）

关注点：
故障检测和预警耗时
故障切换后业务功能的可用性检查
故障切换后数据一致性检查
RPO RTO指标计算

数据恢复测试

测试场景：
中心A故障问题解决后，将中心B的数据恢复到中心A，将中心A恢复为主中心
中心A和中心B的故障问题解决后，将中心C的数据恢复到中心A和中心B，将中心A恢复为主中心

关注点：
数据恢复后，三个中心数据的一致性和完整性检查
主中心恢复后，业务功能可用性检查
RPO RTO指标计算

3.2.2、系统可用性测试
点击图片可查看完整电子表格
3.2.3、数据同步测试
数据同步测试主要分如下两类：


同步类型

验证点

前提条件及数据生成方法

初始数据同步-全量同步

数据一致性
全量同步速度

系统中已经构造好初始数据，具体数据量参加3.2章

持续数据同步-增量同步

数据一致性
增量同步速度
增量同步周期间隔

测试过程中持续运行性能测试脚本，100 路并发，为系统持续提供数据变更

针对每个存储类型，具体的验证方法如下：


存储类型

同步类型

正常同步测试步骤

异常模拟测试步骤

Mysql

全量同步

记录全量同步开始时间。
执行全量数据同步操作，监控数据传输过程中的流量及资源占用情况。
全量同步完成后，使用 mysqldiff --server1=user:password@host1:port1 --server2=user:password@host2:port2 db1.customers:db2.customers 检查数据一致性。
计算全量同步耗时，评估全量同步速度。
检查是否支持指定库名、表名的数据全量同步

在全量同步过程中，模拟网络中断 3分钟，观察同步任务状态及错误日志。
网络恢复后，检查同步任务是否自动恢复并继续执行，直至完成全量同步。
再次使用 mysqldiff 检查数据一致性，确保异常恢复后数据完整且一致。



增量同步
（测试时长-4h）

主库模拟业务高峰期的数据增加、修改、删除操作
主库模拟表的增加、修改、删除操作
主库模拟库的增加、修改、删除操作
检查增量同步的延迟时间，即源库产生数据变更到目标库完成同步的时间间隔，统计平均同步延迟时间。
按一定周期（如每10分钟）执行 mysqldiff 检查增量数据一致性，分析比较结果并记录不一致数据详情。
检查mysql中是否有触发器或者存储过程，是否影响数据同步的准确性
检查同步策略是否为半同步/异步，即中心B或中心C异常后，是否影响中心A的数据写入效率
检查是否支持指定库名、表名的数据增量同步

模拟网络丢包率 20% 持续 3分钟，观察增量同步的受影响情况，检查是否有数据丢失或同步错误记录。
恢复网络正常后，监测增量同步是否能快速恢复正常速率，并且在接下来的几个同步周期内检查数据一致性，确保没有因异常导致的数据不一致问题遗留。

Minio

全量同步

记录全量同步开始时间。
将源存储桶的所有对象复制到目标存储桶，监控传输过程中的网络流量与 Minio 服务器资源使用情况。
完成复制后，通过mc diff检查数据一致性和元数据一致性。
计算全量同步耗时，评估全量同步速度。

全量同步时模拟存储节点故障 3 分钟，查看 Minio 系统的应对机制及同步任务状态。
故障恢复后，检查全量同步是否继续进行并完成，mc diff对比源和目标存储桶对象，验证数据一致性是否受影响。



增量同步
（补充对业务性能的影响检查）

主minio内模拟业务高峰期的对象增加、修改、删除操作。
模拟大文件（>1000M）的增加、修改、删除操作。
模拟大量小文件（<100k）的增加、修改、删除操作。
构造单桶、多桶、一级目录、多级目录等场景。
模拟目录层级的增加、修改、删除操作。
模拟桶的增加、修改、删除操作。
检查增量同步的延迟时间，即源存储桶产生的数据变更到目标存储桶完成同步的时间间隔，统计平均同步延迟时间，评估增量同步速度。
按一定周期（如每 10 分钟）执行mc diff检查增量数据一致性，分析比较结果并记录不一致数据详情。
检查同步策略是否为半同步/异步，即中心B或中心C异常后，是否影响中心A的数据写入效率

模拟网络延迟1秒持续 3 分钟，观察增量同步速率变化及数据传输完整性。
网络恢复后，检查是否有数据积压需要处理，验证数据一致性在异常期间及恢复后是否正常。

Redis

全量同步

待实现方案输出后补充





增量同步

待实现方案输出后补充



Es

全量同步

记录全量数据同步开始时间。
将源es集群数据全量同步到目标es集群，监控同步过程中的资源消耗和网络流量。
全量同步完成后，使用 esdiff或自研脚本，对比数据一致性。
计算全量同步耗时，评估全量同步速度。

全量同步期间模拟集群节点间通信故障 3分钟，检查同步进程状态。
通信恢复后，验证全量同步是否顺利完成且数据一致。



增量同步

源es集群中，模拟业务高峰期的数据增加、修改、删除操作。
模拟索引字段增加、修改、删除操作。
模拟索引增加、修改、删除操作。
检查增量同步的延迟时间，即从源es集群中数据变更到目标es集群完成变更同步的时间，统计平均同步延迟时间，评估增量同步速度。
按一定周期（如每10分钟）检查源es集群和目标es集群间的数据一致性情况。
检查同步策略是否为半同步/异步，即中心B或中心C异常后，是否影响中心A的数据写入效率

模拟磁盘 I/O 过载持续 3分钟，观察对增量同步的影响，包括同步延迟和数据完整性。
磁盘 I/O 恢复正常后，检查增量同步的恢复情况，检查确保数据一致性未受影响。

Mongodb

全量同步

记录全量数据同步开始时间。
将源mongodb集群数据全量同步到目标mongodb集群，监控同步过程中的资源消耗和网络流量。
全量同步完成后，对比源mongodb和目标mongodb的数据一致性。
计算全量同步耗时，评估全量同步速度。

全量同步时模拟数据库连接超时 3 分钟，查看 mongo 相关操作的反馈及同步任务状态。
连接恢复后，检查全量同步是否继续并成功完成，验证数据一致性。



增量同步

源mongodb集群中，模拟业务高峰期的数据增加、修改、删除操作。
模拟表的增加、修改、删除操作。
模拟库的增加、修改、删除操作。
检查增量同步的延迟时间，即从源mongodb集群中数据变更到目标mongodb集群的时间，统计平均同步延迟时间，评估增量同步速度。
按一定周期（如每10分钟）检查源mongodb集群和目标mongodb集群间的数据一致性情况。
检查同步策略是否为半同步/异步，即中心B或中心C异常后，是否影响中心A的数据写入效率

模拟 MongoDB 副本集成员下线 3 分钟，观察增量同步的运行情况及数据同步准确性。
副本集恢复正常后，检查增量同步是否恢复正常节奏，检查数据一致性确保无异常数据残留。

zookeeper

全量同步

待实现方案输出后补充





增量同步

待实现方案输出后补充



doris

全量同步

待实现方案输出后补充





增量同步

待实现方案输出后补充



3.2.4、故障切换演练


故障分类

测试场景

验证点

单节点故障
（AB->AB）

在中心A内模拟部分机器宕机
在中心A内模拟部分组件挂掉

中心A高可用架构对单节点故障的处理能力
预警及时性与准确性
组件自拉起及时性与拉起后的可用性

公共组件灾备切换

mysql灾备切换
es灾备切换
minio灾备切换
mongodb灾备切换
doris灾备切换
zookeeper灾备切换
redis灾备切换

公共组件异常告警的及时性与准确性
公共组件灾备切换耗时统计
公共组件灾备切换后，读写可用性检查和数据一致性检查

同城灾备切换
（AB->B）

中心A数据故障，切换到中心B
中心A应用故障，切换到中心B
中心A机房断电，切换到中心B

中心A故障预警的及时性与准确性
中心A切换到中心B，业务是否运行正常
中心A切换到中心B，数据是否从中心B同步到中心C，中心B和中心C数据一致
同城灾备切换RPO统计：发生故障时间 - 中心A最后一次数据同步给中心B的时间
同城灾备切换RTO统计：切换到中心B业务恢复时间 - 中心A故障发生时间

异地灾备切换
（AB->C）

中心A和中心B机房均断电，切换到中心C

中心A和中心B预警的及时性与准确性
中心A切换到中心C，业务是否运行正常
异地灾备切换RPO统计：发生故障时间 - 中心A最后一次数据同步给中心C的时间
异地灾备切换RTO统计：切换到中心C业务恢复时间 - 中心A故障发生时间

中心B故障
（AB->A）

中心B数据故障，预警后增量同步恢复
中心B应用故障，业务入口删除中心B地址
中心B机房断电，业务入口删除中心B地址
中心B和中心A之间网络故障，业务入口删除中心B地址

中心B预警的及时性与准确性
中心B停止后，中心A独立提供服务，业务正常可用
中心C的数据和中心A的数据保持一致
中心B的网络故障、数据故障修复后，中心A可将故障期间的数据同步到中心B
中心B预警的及时性与准确性
RPO统计：0，因中心B的数据是冷备，故障了不会导致数据丢失，可以从中心A同步过来
RTO统计：中心A独立提供服务时间 - 中心B故障发生时间

中心C故障
（AB->AB）

中心C数据故障，预警后增量同步恢复
中心C机房断电，恢复供电后增量同步数据
中心C和中心A之间网络故障，网络恢复后增量同步数据
中心C和中心B之间网络故障，对业务和数据同步无影响

中心C预警的及时性与准确性
中心C故障恢复后，验证和中心A的数据一致性
3.2.4.1、单节点故障演练
参照产品高可用测试用例，在中心A中模拟机器宕机和进程挂掉的情况
服务机器人平台高可用测试用例

3.2.4.2、公共组件故障演练


故障分类

测试场景

验证点

mysql灾备切换

mysql master异常
mysql slave异常
mysql proxySQL异常

mysql主节点异常告警延时统计
mysql主从节点切换耗时统计
proxySQL异常告警延时统计
proxySQL异常恢复耗时统计
mysql主从切换后，读写可用性检查和数据一致性检查

es灾备切换

es master异常
es slave异常
Logstash异常

es主节点异常告警延时统计
es主从节点切换耗时统计
Logstash异常告警延时统计
Logstash异常恢复耗时统计
es主从切换后，读写可用性检查和数据一致性检查

minio灾备切换

minio master异常
minio slave异常

minio主节点异常告警延时统计
minio主从切换耗时统计
minio主从切换后，读写可用性检查和数据一致性检查

mongodb灾备切换

mongodb master异常
mongodb slave异常

mongodb主节点异常告警延时统计
mongodb主从切换耗时统计
mongodb主从切换后，读写可用性检查和数据一致性检查

zookeeper灾备切换

待研发输出实现方案后补充



doris灾备切换

待研发输出实现方案后补充



redis灾备切换

待研发输出实现方案后补充



3.2.4.3、同城灾备切换
点击图片可查看完整电子表格
以下是minio故障演练的具体步骤：
准备工作
参照第3.1.1章和第3.1.2章，完成三中心环境部署和数据初始化，运行性能测试脚本

故障模拟之minio异常
Step 1：
登录中心A minio所在机器，模拟磁盘写满，记录故障时间t1
./blade c disk fill --path /data --percent 100 --retain-handle --timeout 6000

Step 2：
检查是否收到星际发出的告警邮件，记录告警时间t2

Step 3：
登录中心A的nacos服务，修改srp-common.properties中的s3.endpoint配置项
image38.png


Step 4：
登录中心B的nacos服务，修改srp-common.properties中的s3.endpoint配置项，记录配置修改完成时间t3，此时系统可正常提供minio数据的写入功能
image38.png


Step 5：
登录中心A的nginx服务器，修改minio-server地址，reload配置
image39.png


Step 6：
登录中心B的nginx服务器，修改minio-server地址，reload配置
image39.png


Step 7：
登录采编服务所在的服务器，修改docqa/conf/parse/application.yml的minio endpoint配置项，重启docuparse组件。重启成功后，记录minio切换完成时间t4
image40.png


Step 8：
分别访问中心A和中心B的服务机器人地址，检查历史素材知识是否能在线预览，是否能成功采编发布新的素材知识。记录服务验证可用时间t5

Step 9：
修改/检查minio异地灾备同步脚本，确认数据是否正常从中心B的minio同步到中心C的minio。停止性能测试脚本，对比中心C和中心B的minio数据一致性

Step 10：
检查故障发生前，最后一次中心A同步数据到中心B的时间，记作t6
输出指标RPO = t3 - t6（minio可写入时间-最后一次成功同步到中心B的时间）
输出指标RTO = t5 - t1（服务恢复时间-故障发生时间）

3.2.4.4、异地灾备切换
点击图片可查看完整电子表格

3.2.4.5、异地中心故障
点击图片可查看完整电子表格


3.2.5、数据恢复测试


测试场景

验证点

中心B数据恢复到中心A，将A切换回主中心

服务机器人组件及依赖的CBB组件，数据存储的主键生成逻辑检查，不存在主键自增生成的情况
支持在线查看数据一致性和冲突数据列表，且支持在线处理冲突数据
中心A、中心B和中心C之间数据一致性检查
数据恢复+主中心切换耗时统计
数据恢复+主中心切换后，系统可用性检查
主中心切换后，中心A是否可正常同步数据到B和C

中心C数据恢复到中心A，将A切换回主中心



3.3、测试结果
测试完成后，输出各场景下的RTO和RPO的值，如下表格：


故障分类

RPO

RTO

测试通过标准

mysql灾备切换

T1 = master最后一次数据同步到slave的时间
T2 = 发生故障时间
RPO = T2 - T1

T2 = 发生故障时间
T3 = master成功切换的时间
RTO = T3 - T2

RTO<24 小时RPO<120 分钟

es灾备切换







minio灾备切换







mongodb灾备切换







zookeeper灾备切换







doris灾备切换







redis灾备切换







同城灾备切换（AB->B）

T1 = 中心A最后一次数据同步到中心B的时间
T2 = 发生故障时间
RPO = T2 - T1

T2 = 发生故障时间
T3 = 中心B成功接管业务时间
RTO = T3 - T2



同城灾备切换（AB->A）

0

T2 = 发生故障时间
T3 = 中心A独立提供业务服务时间
RTO = T3 - T2



异地灾备切换（AB->C）

T1 = 中心A最后一次数据同步到中心C的时间
T2 = 发生故障时间
RPO = T2 - T1

T2 = 发生故障时间
T3 = 中心C成功接管业务时间
RTO = T3 - T2



异地中心故障（AB->AB）

0

0


4、工作量评估
初步评估，测试工作量共154人日，约7人月


工作类别

工作内容

工作量（人日）

测试设计

两地三中心测试用例编写

10

测试准备

准备测试环境，搭建两地三中心服务机器人整套环境

10



准备7个公共组件的数据一致性对比测试工具和脚本

7



准备测试数据

3

测试执行
按三轮迭代评估

组件级-mysql数据同步、主备切换和数据恢复测试

10



组件级-es数据同步、主备切换和数据恢复测试

10



组件级-minio数据同步、主备切换和数据恢复测试

10



组件级-mongodb数据同步、主备切换和数据恢复测试

10



组件级-doris数据同步、主备切换和数据恢复测试

10



组件级-zookeeper数据同步、主备切换和数据恢复测试

10



组件级-redis数据同步、主备切换和数据恢复测试

10



系统级-单节点故障演练测试

15



系统级-同城灾备切换演练测试

10



系统级-异地灾备切换演练测试

10



系统级-异地中心故障演练测试

6



系统级-数据恢复测试

10

测试报告

测试数据分析、结果整理，输出测试报告

3
5、输出产物
《服务机器人两地三中心实现方案》
《服务机器人两地三中心测试方案和测试工具脚本》
《服务机器人两地三中心故障切换指导手册》
《服务机器人两地三中心测试报告》
服务机器人两地三中心数据一致性检查和补偿服务，支持在线统一检查和处理各公共组件的数据一致性情况
