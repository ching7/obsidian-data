---
created: '2025-03-19 17:11:53'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/YnrudPCVyowAIexHkI1cF3xjnwb
modified: '2025-03-28 16:40:44'
source: feishu
title: å°å‹å¤§æ¨¡å‹ï¼ˆLLMï¼‰æ•´ä½“æ–¹æ¡ˆ20250319
---

å°å‹å¤§æ¨¡å‹ï¼ˆLLMï¼‰æ•´ä½“æ–¹æ¡ˆ20250319
è¦å®ç°ä¸€ä¸ªå®Œæ•´çš„å°å‹å¤§æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒæ—¶è¦†ç›–å¤§æ¨¡å‹çš„å‘å±•å†å²å’Œå…³é”®æŠ€æœ¯ï¼Œæˆ‘ä¼šä¸ºä½ è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„å­¦ä¹ å’Œå®ç°è·¯å¾„ã€‚è¿™ä¸ªè·¯å¾„å°†ä»æœ€åŸºç¡€çš„è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¼•å…¥æ›´å¤æ‚çš„æœºåˆ¶ï¼ˆå¦‚ RNNã€LSTMã€Transformer ç­‰ï¼‰ï¼Œæœ€ç»ˆæ­å»ºå‡ºä¸€ä¸ªç®€åŒ–ç‰ˆçš„ LLMï¼Œè¦†ç›–ä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µï¼š

ğŸ† é¡¹ç›®ç›®æ ‡
âœ… ç”¨ Python é€æ­¥å®ç°ä¸€ä¸ªå°å‹çš„è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬æ¨ç†å’Œè®­ç»ƒï¼‰
 âœ… åœ¨æ¨¡å‹ä¸­ä½“ç°å¤§æ¨¡å‹çš„å…³é”®æŠ€æœ¯ï¼ˆå¦‚ Attentionã€ä½ç½®ç¼–ç ã€Tokenization ç­‰ï¼‰
 âœ… å­¦ä¹ å¤§æ¨¡å‹çš„å‘å±•å†ç¨‹ï¼ˆä» RNN â†’ LSTM â†’ Transformer â†’ GPTï¼‰
 âœ… äº†è§£å¹¶å®ç°åŸºç¡€ NLP ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ï¼‰

ğŸ† å®Œæ•´å­¦ä¹ è·¯å¾„
ğŸš€ é˜¶æ®µ1ï¼šç†è§£è¯­è¨€æ¨¡å‹åŸºç¡€
âœ… äº†è§£å¤§æ¨¡å‹å‘å±•å†å²
 âœ… ä»æœ€åŸºç¡€çš„ N-Gram æ¨¡å‹å…¥æ‰‹ï¼š N-Gramæ¨¡å‹
 âœ… å­¦ä¹  Tokenizationï¼ˆåˆ†è¯ï¼‰å’Œ Word Embeddingï¼ˆè¯åµŒå…¥ï¼‰02 Tokenizationï¼ˆåˆ†è¯ï¼‰å’Œ Word Embeddingï¼ˆè¯åµŒå…¥ï¼‰
 âœ… æ‰‹åŠ¨å®ç°ä¸€ä¸ªç®€å•çš„ N-Gram è¯­è¨€æ¨¡å‹
ğŸ‘‰ å®ç°ä»»åŠ¡ï¼š
ç”¨ Python å®ç°ä¸€ä¸ªåŸºäº N-Gram çš„æ–‡æœ¬ç”Ÿæˆå™¨
ğŸ”¥ ç¤ºä¾‹ä»£ç ï¼ˆN-Gramï¼‰

from collections import defaultdict
import random

# è®­ç»ƒæ–‡æœ¬
text = "I love this product very much. I love this shop. I hate bad products."

# åˆ†è¯
words = text.split()


# åˆ›å»º n-gram æ¨¡å‹ï¼ˆæ”¯æŒ Unigram, Bigram, Trigram, 4-gramï¼‰
def create_ngram_model(words, n):
    ngram_model = defaultdict(list)
    for i in range(len(words) - n + 1):
        ngram = tuple(words[i:i + n])  # ç”Ÿæˆ n-gram
        ngram_model[ngram[:-1]].append(ngram[-1])  # n-gram çš„å‰éƒ¨åˆ†ä½œä¸ºé”®ï¼Œæœ€åä¸€ä¸ªå•è¯ä½œä¸ºå€¼
    return ngram_model


# æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼šè¯æ±‡è¡¨å¤§å°
V = len(set(words))  # è¯æ±‡è¡¨å¤§å°


# è®¡ç®— n-gram æ¦‚ç‡
def ngram_probability(model, ngram_prefix, next_word):
    count_ngram = model[ngram_prefix].count(next_word)
    count_prefix = len(model[ngram_prefix])
    return (count_ngram + 1) / (count_prefix + V)


# ç”Ÿæˆ n-gram æ–‡æœ¬
def generate_ngram_text(model, n, start_word, length=10):
    # å…¶ä»– n-gram æ¨¡å‹ï¼šé€šè¿‡ n-gram å‰ç¼€ç”Ÿæˆä¸‹ä¸€ä¸ªè¯
    current_ngram = tuple([start_word] * (n - 1))  # æ ¹æ®æ¨¡å‹å¤§å°åˆå§‹åŒ–
    result = [start_word]
    for _ in range(length):
        if model[current_ngram]:
            next_word = random.choices(model[current_ngram],
                                       weights=[ngram_probability(model, current_ngram, word) for word in
                                                model[current_ngram]])[0]
            result.append(next_word)
            current_ngram = tuple(list(current_ngram[1:]) + [next_word])  # æ»šåŠ¨æ›´æ–° n-gram
    return ' '.join(result)


# åˆ›å»º Unigram, Bigram, Trigram å’Œ 4-gram æ¨¡å‹
unigram_model = create_ngram_model(words, 1)
bigram_model = create_ngram_model(words, 2)
trigram_model = create_ngram_model(words, 3)
fourgram_model = create_ngram_model(words, 4)

# ç”Ÿæˆä¸åŒ n-gram æ–‡æœ¬
print("Generated Sentence with Unigram Model:")
print(generate_ngram_text(unigram_model, 1, "I"))

print("\nGenerated Sentence with Bigram Model:")
print(generate_ngram_text(bigram_model, 2, "I"))

print("\nGenerated Sentence with Trigram Model:")
print(generate_ngram_text(trigram_model, 3, "I"))

print("\nGenerated Sentence with 4-gram Model:")
print(generate_ngram_text(fourgram_model, 4, "I"))

ğŸš€ é˜¶æ®µ2ï¼šä» RNN åˆ° LSTM
âœ… å­¦ä¹  RNNï¼ˆRecurrent Neural Networkï¼‰çš„ç»“æ„
 âœ… ç†è§£ RNN çš„é—®é¢˜ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
 âœ… å­¦ä¹  LSTMï¼ˆLong Short-Term Memoryï¼‰çš„æ”¹è¿›æœºåˆ¶
 âœ… å®ç°ä¸€ä¸ªå°å‹ RNN å’Œ LSTM è¯­è¨€æ¨¡å‹
ğŸ‘‰ å®ç°ä»»åŠ¡ï¼š
å®ç°ä¸€ä¸ªåŸºäº RNN çš„æ–‡æœ¬ç”Ÿæˆå™¨
ç”¨ LSTM è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜
ğŸ”¥ ç¤ºä¾‹ä»£ç ï¼ˆLSTMï¼‰

import torch
import torch.nn as nn
å®šä¹‰ LSTM æ¨¡å‹
class LSTMModel(nn.Module):
    def 
__init__
(self, vocab_size, embedding_dim, hidden_dim):
        super(LSTMModel, self).
__init__
()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    def forward(self, x):
        x = self.embedding(x)
        output, (hidden, cell) = self.lstm(x)
        output = self.fc(output)
        return output
åˆå§‹åŒ–æ¨¡å‹
vocab_size = 1000
embedding_dim = 128
hidden_dim = 256
model = LSTMModel(vocab_size, embedding_dim, hidden_dim)
æµ‹è¯•è¾“å…¥
test_input = torch.randint(0, vocab_size, (10, 5)) # batch_size=10, sequence_length=5
output = model(test_input)
print(output.shape) # è¾“å‡º: [10, 5, vocab_size]

ğŸš€ é˜¶æ®µ3ï¼šå¼•å…¥ Transformer
âœ… å­¦ä¹  Transformer ç»“æ„ï¼ˆAttentionã€Position Encodingã€Feed Forwardï¼‰
 âœ… å®ç°ä¸€ä¸ªå°å‹ Transformer
 âœ… ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰
ğŸ‘‰ å®ç°ä»»åŠ¡ï¼š
ç”¨ PyTorch å®ç°ä¸€ä¸ªæœ€å°åŒ– Transformer
ç”¨ Transformer ç”Ÿæˆæ–‡æœ¬
ğŸ”¥ ç¤ºä¾‹ä»£ç ï¼ˆTransformerï¼‰

import torch
import torch.nn as nn
import torch.optim as optim
å®šä¹‰ Transformer æ¨¡å‹
class TransformerModel(nn.Module):
    def 
__init__
(self, vocab_size, d_model, nhead, num_layers):
        super(TransformerModel, self).
__init__
()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_layers)
        self.fc = nn.Linear(d_model, vocab_size)
    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x, x)
        return self.fc(x)
vocab_size = 1000
d_model = 128
nhead = 8
num_layers = 2
model = TransformerModel(vocab_size, d_model, nhead, num_layers)
test_input = torch.randint(0, vocab_size, (10, 5)) # batch_size=10, sequence_length=5
output = model(test_input)
print(output.shape) # è¾“å‡º: [10, 5, vocab_size]

ğŸš€ é˜¶æ®µ4ï¼šæ­å»ºä¸€ä¸ªå°å‹ GPT æ¨¡å‹
âœ… ç†è§£è§£ç å™¨ç»“æ„ï¼ˆDecoderï¼‰
 âœ… ä½¿ç”¨ Masked Attention
 âœ… è®­ç»ƒå’Œæ¨ç†
ğŸ‘‰ å®ç°ä»»åŠ¡ï¼š
å®ç°ä¸€ä¸ªå°å‹ GPTï¼ˆè§£ç å™¨ç»“æ„ï¼‰
è®­ç»ƒå¹¶ç”Ÿæˆæ–‡æœ¬
ğŸ”¥ ç¤ºä¾‹ä»£ç ï¼ˆGPTï¼‰

import torch
import torch.nn as nn
class GPTModel(nn.Module):
    def 
__init__
(self, vocab_size, d_model, nhead, num_layers):
        super(GPTModel, self).
__init__
()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead),
            num_layers
        )
        self.fc = nn.Linear(d_model, vocab_size)
    def forward(self, x, tgt_mask):
        x = self.embedding(x)
        x = self.transformer(x, x, tgt_mask)
        return self.fc(x)
vocab_size = 1000
d_model = 128
nhead = 8
num_layers = 2
model = GPTModel(vocab_size, d_model, nhead, num_layers)
test_input = torch.randint(0, vocab_size, (10, 5))
mask = torch.triu(torch.ones(5, 5) * float('-inf'), diagonal=1)
output = model(test_input, mask)
print(output.shape)  # è¾“å‡º: [10, 5, vocab_size]

ğŸš€ é˜¶æ®µ5ï¼šä¼˜åŒ–ä¸å¾®è°ƒ
âœ… å­¦ä¹ æŸå¤±å‡½æ•°ï¼ˆCrossEntropyï¼‰
 âœ… ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
 âœ… å¼•å…¥æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–
 âœ… ç”¨å°æ•°æ®é›†è®­ç»ƒ
ğŸ‘‰ å®ç°ä»»åŠ¡ï¼š
è®­ç»ƒä¸€ä¸ªå°å‹ GPT æ¨¡å‹
ä¼˜åŒ–è®­ç»ƒæ•ˆæœ

ğŸ† æœ€ç»ˆç›®æ ‡
ğŸ”¹ å®Œæˆä¸€ä¸ªæœ€å°åŒ–è¯­è¨€æ¨¡å‹ï¼ˆç±»ä¼¼ GPT-2 çš„å°ç‰ˆæœ¬ï¼‰
 ğŸ”¹ ç”Ÿæˆç®€å•çš„æ–‡æœ¬ï¼ˆå¯¹è¯ã€æ•…äº‹ç”Ÿæˆç­‰ï¼‰
 ğŸ”¹ ç†è§£å¤§æ¨¡å‹å‘å±•å†ç¨‹å’ŒèƒŒåçš„åŸç†

ğŸ¯ æ•´ä½“æ¡†æ¶
âœ… N-Gram â” RNN â” LSTM â” Transformer â” GPT
 âœ… åœ¨æ¯ä¸ªé˜¶æ®µéƒ½è¦æœ‰å®Œæ•´çš„ä»£ç å®ç°
 âœ… å®Œæ•´è®­ç»ƒ + æ¨ç†æµç¨‹

ğŸ’¡ ğŸ‘‰ ä¸‹ä¸€æ­¥æ˜¯ä»å“ªéƒ¨åˆ†å¼€å§‹ï¼Ÿè¦è¯¦ç»†å±•å¼€å“ªä¸€æ®µï¼Ÿ ğŸ˜
