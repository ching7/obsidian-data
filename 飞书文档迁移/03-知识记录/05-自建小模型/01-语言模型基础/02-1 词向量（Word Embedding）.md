---
created: '2025-03-28 16:46:51'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/VTlKdIhtbohbqkxb7t3c38ZBngc
modified: '2025-03-28 16:47:45'
source: feishu
title: 02-1 词向量（Word Embedding）
---

02-1 词向量（Word Embedding）
词向量（Word Embedding）简介
词向量（Word Embedding）是将词语映射到一个固定长度的稠密向量空间中的技术。相比传统的 One-hot 编码，词向量能够有效地表示词之间的语义关系，从而使得神经网络等模型能够更好地处理自然语言任务。
词向量的关键特点：
通过向量的高维空间表示词语，能够捕捉到词与词之间的相似性和语法关系。
向量通常是低维的，具有较小的维度，并且向量之间的关系能够反映词语之间的语义相似度。
常见的词向量方法包括 One-hot 编码、Word2Vec、GloVe 和 FastText，下面我们将逐一介绍这四种方法，并给出具体的例子和形象的解释。

One-hot 编码
定义：
 One-hot 编码是将每个词转换为一个高维的稀疏向量。在这个向量中，只有一个元素是 1，其余元素都是 0，表示词汇表中该词对应的索引。
特点：
高维且稀疏。
无法表示词与词之间的关系。例如，dog 和 cat 这两个词在 One-hot 编码中是完全不同的，虽然它们语义上有相似性。
例子：
 假设我们的词汇表有 5 个单词：["I", "love", "dog", "cat", "play"]，那么它们的 One-hot 编码如下：
"I" → [1, 0, 0, 0, 0]
"love" → [0, 1, 0, 0, 0]
"dog" → [0, 0, 1, 0, 0]
"cat" → [0, 0, 0, 1, 0]
"play" → [0, 0, 0, 0, 1]
形象解释：
你可以把 One-hot 编码想象成一个很大的表格，词汇表中的每个词都是一个单独的列，只有一个单词对应的位置为 1，其余的位置全是 0。

Word2Vec
定义：
 Word2Vec 是一种基于深度学习的词向量模型，通过上下文信息学习词语的稠密向量表示。Word2Vec 有两种模型：CBOW（Continuous Bag of Words） 和 Skip-gram。
CBOW：通过上下文词语预测目标词。
Skip-gram：通过目标词预测上下文词语。
特点：
低维度的稠密向量，能够捕捉到词与词之间的语义相似性。
通过语料库中的上下文关系学习词的表示。
例子：
 假设我们有如下语句：“I love natural language processing”
 使用 Word2Vec 后，我们会得到每个词的词向量，例如：
"I" → [0.23, -0.56, 0.77]
"love" → [0.45, -0.12, 0.34]
"natural" → [-0.21, 0.53, 0.68]
"language" → [0.32, 0.19, -0.08]
"processing" → [0.12, 0.44, -0.13]
这些向量不仅是数字，还能够反映词语之间的关系，例如 love 和 like 可能会有相似的向量。
形象解释：
可以将 Word2Vec 想象为一个“地图”，每个词都有一个坐标（向量），通过该地图可以看出哪些词在语义上更接近，哪些词则差距较大。

GloVe (Global Vectors for Word Representation)
定义：
 GloVe 是基于全局统计信息的词向量表示方法。与 Word2Vec 不同，GloVe 是通过词与词之间的共现概率来学习词向量，而不是通过上下文窗口的方式。
特点：
利用全局语境（全局统计信息）来构建词向量，重点是词的共现频率。
生成的词向量能够更好地表示全局的语义关系。
例子：
 假设我们有两个词 king 和 queen，通过 GloVe 训练后，它们的词向量可能如下：
"king" → [0.45, -0.22, 0.89]
"queen" → [0.43, -0.24, 0.91]
这些词向量表示了它们的语义相似性，GloVe 方法特别善于捕捉全局语境。
形象解释：
你可以把 GloVe 想象为一个“大数据表”，在这个表中，每一对词都有一个共现频率，而 GloVe 会根据这些频率来计算每个词的向量，反映出这些词在大语料库中的关联程度。

FastText
定义：
 FastText 是 Facebook 提出的词向量模型，它的特点是不仅仅表示词的整体，而是考虑了词内的 子词（subword） 信息。也就是说，它通过将词拆分成子词或字符来生成词向量，从而能更好地处理未见过的词（OOV）。
特点：
考虑了词内部的子词信息，能更好地处理未知词。
对形态变化较大的语言（如德语、芬兰语等）尤其有效。
例子：
 对于单词 apple，FastText 会将其拆解为多个子词（如：a, p, p, l, e），并通过这些子词的向量来生成单词的最终向量。因此，即使遇到未知词，FastText 也能通过其子词信息来推断词向量。
"apple" → [0.32, 0.57, -0.22]
"appl" → [0.21, 0.45, -0.19]
"ple" → [0.14, 0.38, -0.05]
形象解释：
你可以把 FastText 看作一个“字母拼图”，通过不同的拼图块（子词或字符）组合成最终的词向量。这样，即使是没有见过的词，它也能通过拼图块来推测出一个合适的向量。

总结：
One-hot 编码：通过将每个词映射到一个高维稀疏向量来表示。虽然简单，但无法捕捉词之间的语义关系。
Word2Vec：通过上下文信息学习词的稠密向量，能够反映词语之间的相似性。
GloVe：基于词的共现统计信息来学习词向量，更关注全局语境。
FastText：通过子词信息表示词向量，能够更好地处理未见过的词和形态变化较大的语言。
这些方法各有优劣，选择哪种方法取决于任务需求及语料库的特性。

可以把向量的维度看作为从词语的不同维度来衡量词语间的关联吗
是的，向量的维度可以被理解为从不同的特征维度来衡量词语之间的关联。每个维度代表词语在某个方面的语义或语法特征，而词语之间的相似性则体现在它们的向量在这些维度上的接近程度。
形象理解：
可以将词向量的每个维度看作是一个特征，这些特征共同定义了一个词的“意义”。而词语的向量之间的距离（例如余弦相似度）则反映了它们在这些特征维度上的相似性。
举个例子，假设我们有一个非常简单的词向量模型，其中有 3 个维度：
维度 1：表示词的“情感”强度（例如正向或负向情感）。
维度 2：表示词的“动作/状态”特征（例如是否动词）。
维度 3：表示词的“类别”特征（例如名词、动词等）。
举个例子：
假设我们有以下几个词及其对应的向量：
love → [0.8, 0.1, 0.5]
hate → [-0.7, 0.2, 0.5]
run → [0.1, 0.9, 0.2]
dog → [0.2, 0.1, 0.9]
这些向量的每个维度分别代表情感强度、动作特征和类别特征：
love 和 hate 在情感维度上有较大的差异，但它们在动作/状态维度和类别维度上差异不大，因此它们的向量是相对接近的（虽然情感维度上相反，但其他维度相似）。
run 和 love 在“动作/状态”特征上差异很大，而 dog 和 love 在“类别”特征上相似，导致它们的向量位置在不同的“语义空间”中。
向量的维度如何衡量词语间的关联：
维度的作用：每个维度代表一个独立的特征，词语的向量可以通过多个维度来表达词的不同方面（如情感、类别、动作等）。
相似性度量：词语的向量越接近，说明它们在多个特征维度上的表现越相似。因此，词向量的相似度（比如通过余弦相似度计算）可以衡量词语在这些特征维度上的相似性。
进一步的理解：
低维向量表示：在 Word2Vec 或 GloVe 等词向量模型中，词语的向量通常会有几十到几百维。每个维度并没有显式的语义解释，而是通过大量语料中的统计信息来学习词与词之间的语义关系。
比如，一个 3 维 的词向量可能无法明确地表示情感、动作、类别等特征，但在更高维的向量空间中，词语的各个语义特征被分布到不同的维度上，使得词向量能够有效地捕捉复杂的语义信息。
学习到的语义关系：虽然我们不能直接解释每个维度的含义，但通过训练，模型会自动学习到有意义的表示，使得相似的词语在向量空间中尽可能接近。例如，“king”和“queen”的向量会非常接近，它们在某些维度上的数值差异也可能反映出“性别”这一特征。
小结：
是的，向量的维度可以看作是衡量词语不同特征（语义、语法等）的一种方式。
通过学习这些维度，词向量模型能够捕捉词语之间的复杂关系，并在高维空间中实现相似词的接近。因此，词语之间的关联可以通过向量在各个维度上的位置关系来衡量。
