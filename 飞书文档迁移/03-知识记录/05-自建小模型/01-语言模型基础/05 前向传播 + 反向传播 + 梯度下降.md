---
created: '2025-03-20 14:27:50'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/XmeCdUaAgoudY1xi5PMc4TX3nGb
modified: '2025-03-21 17:35:17'
source: feishu
title: 05 前向传播 + 反向传播 + 梯度下降
---

05 前向传播 + 反向传播 + 梯度下降
明白！那我们就一步步地从最基础的概念出发，详细解释每个步骤，并给出具体示例。
 💡 我们最终要实现的任务：
通过一个简单的2 层神经网络学习加法和减法
用深度学习（基于梯度下降）和强化学习（基于奖励机制）来优化

🏆 1. 什么是神经网络？
神经网络（Neural Network）是模拟人脑神经元工作机制的数学模型。
人脑中有：
神经元（Neuron）
突触（Synapse）：连接神经元，传递信号
在神经网络中对应为：
神经元（Neuron） → 数学运算单元
突触（Synapse） → 权重（Weight）
信号传递（Signal） → 激活函数（Activation）

🧠 2. 两层神经网络结构
👉 我们实现的神经网络有以下结构：


🔎 3. 前向传播（Forward Propagation）
前向传播的核心操作是：
 
✅ 步骤：
输入数据通过权重（weight）和偏置（bias）加权求和
通过激活函数进行非线性转换
输出到下一层

🔥 代码实现：前向传播
定义一个两层的神经网络👇

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 定义一个简单的2层神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        # 调用父类的构造函数
        super().__init__()
        # 输入层 → 隐藏层（2 -> 5）
        self.hidden = nn.Linear(2, 5)
        # 隐藏层 → 输出层（5 -> 1）
        self.output = nn.Linear(5, 1)
        # 激活函数（ReLU）
        self.relu = nn.ReLU()

    def forward(self, x):
        # 输入通过隐藏层，进行线性加权求和
        x = self.hidden(x)
        # 通过激活函数，进行非线性转换
        x = self.relu(x)
        # 输出层直接输出
        return self.output(x)

# 定义模型
model = SimpleNN()

🧩 解释：
nn.Linear(2, 5) → 线性层，输入 2 维 → 输出 5 维
2 → 输入两个数（比如加法的两个数）
5 → 输出 5 个特征（学习到的特征表示）
nn.ReLU() → 激活函数
把线性输出转换成非线性表示
nn.Linear(5, 1) → 输出层
将隐藏层的 5 个特征合成为 1 个输出（最终结果）

🌟 4. 激活函数（Activation Function）
👉 为什么需要激活函数？
如果不使用激活函数，模型就是一个“线性模型”（即只能学习加权和）
使用激活函数引入非线性，使模型能够学习更复杂的模式
✅ ReLU（Rectified Linear Unit）激活函数
06 激活函数
ReLU 定义为：
 
优点：
 ✅ 计算速度快
 ✅ 防止梯度消失
 ✅ 保持稀疏激活

🎯 示例：ReLU 激活

import torch
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
relu = nn.ReLU()
output = relu(x)
print(output)  # 输出：[0, 0, 0, 1, 2]

🚀 5. 损失函数（Loss Function）
损失函数用于衡量模型的输出与真实值的误差。
✅ 均方误差（MSE）
定义为：
 
  = 预测值
  = 真实值

🔥 代码实现：损失函数
09 损失函数

criterion = nn.MSELoss()

🌟 6. 反向传播（Backward Propagation）
反向传播的核心操作是：
计算损失
通过梯度下降更新权重

✅ 梯度下降公式
 
  = 学习率（Learning Rate）
  = 损失对参数的导数，涉及偏导数10 导数与偏导数

🔥 代码实现：反向传播

optimizer = optim.SGD(model.parameters(), lr=0.01)
训练循环
for epoch in range(100):
    # 1. 前向传播
    output = model(x)

    # 2. 计算损失
    loss = criterion(output, y)

    # 3. 清空上一次的梯度
    optimizer.zero_grad()

    # 4. 反向传播，计算梯度
    loss.backward()

    # 5. 更新参数
    optimizer.step()

✅ 7. 训练 + 预测（完整代码）

x = torch.tensor([[5, 3]], dtype=torch.float32)
y = torch.tensor([[8]], dtype=torch.float32)
for epoch in range(1000):
    output = model(x)
    loss = criterion(output, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % 100 == 0:
        print(f"Epoch [{epoch}/1000], Loss: {loss.item():.4f}")
预测
test_input = torch.tensor([[5, 3]], dtype=torch.float32)
predicted = model(test_input).item()
print(f"5 + 3 = {predicted:.2f}")

🌈 8. 过程总结


🚀 💡 深度学习的核心在于：前向传播 + 反向传播 + 梯度下降
👉 接下来，我们可以在此基础上，引入强化学习部分！ 😎
