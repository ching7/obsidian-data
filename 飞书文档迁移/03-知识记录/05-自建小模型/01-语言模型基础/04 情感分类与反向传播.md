---
created: '2025-03-20 10:11:32'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/V2nAdcWM0owDvpxkzzNca1tMnKe
modified: '2025-03-20 16:56:09'
source: feishu
title: 04 情感分类与反向传播
---

04 情感分类与反向传播
🌟 1. 如何知道什么是正面情感？
判断正面或负面情感（Sentiment Analysis）通常是通过有监督学习来完成的。基本流程是：
准备带标签的数据集：每个样本（文本）都标注好是正面（positive）还是负面（negative）。
模型学习模式：让神经网络通过训练来学习哪些词更可能出现在正面或负面情感中。
通过词向量和上下文关系：模型通过学习词向量和上下文，识别哪些词与正面或负面情感相关联。

✅ （1）正面情感 VS 负面情感示例
假设我们有一个情感分类任务，训练数据如下：


✅ （2）词向量中学习到的模式
如果模型在很多正面句子中看到“love”“amazing”等词 → 可能学习到这些词与正面情感相关
如果模型在很多负面句子中看到“hate”“terrible”等词 → 可能学习到这些词与负面情感相关

✅ （3）通过向量间的距离学习情感模式
训练后，模型可能学到以下的词向量分布：
正面词的向量可能更接近彼此（如 love, happy, amazing）
负面词的向量可能也更接近彼此（如 hate, terrible, bad）
例子（假设向量为 3 维）：

love → [0.45, -0.23, 0.78]  
hate → [-0.67, 0.12, -0.56]  
amazing → [0.34, -0.12, 0.67]  
terrible → [-0.45, 0.23, -0.78]  
在向量空间中，正面词和负面词会自动聚集在不同的区域：
Cosine Similarity（余弦相似度）可以用来衡量向量之间的相似度
余弦相似度公式：
 

✅ （4）为什么神经网络能识别正负情感？
通过词向量学习词与情感之间的模式
通过上下文（N-Gram 或 Transformer Attention）理解句子结构
通过反向传播调整模型参数，让模型更精准地学习上下文和情感模式

🚀 2. 反向传播（Back Propagation）
反向传播（Back Propagation）是神经网络中最核心的训练方法，用于：
根据模型的误差（Loss）来调整权重
通过梯度下降（Gradient Descent）来最小化损失

✅ （1）基本思路
前向传播（Forward Propagation）
输入：词向量
经过隐藏层 → 输出结果
损失计算（Loss Calculation）
通过交叉熵损失（Cross Entropy）或均方误差（MSE）等计算误差
反向传播（Back Propagation）
通过误差的梯度计算，调整每一层的参数
使用链式法则计算每一层的损失对权重的导数
权重更新
使用梯度下降更新权重

🏆 （2）反向传播公式
损失函数（Loss Function）：
 
其中：
 = 真实标签（1 表示正面，0 表示负面）
  = 预测概率

反向传播中的核心步骤：
对损失函数   求关于每个参数  的梯度：
 
用梯度下降法更新参数：
 
其中：
  = 学习率（Learning Rate）

🎯 （3）完整流程
前向传播
损失计算
反向传播
更新参数
重复训练 → 直到损失函数收敛

🔥 （4）示例代码
使用 PyTorch 训练一个简单的情感分类模型，使用反向传播来调整参数：

import torch
import torch.nn as nn
import torch.optim as optim
#输入向量 (2 个样本，每个样本 3 维特征)
X = torch.tensor([[0.45, -0.23, 0.78], [-0.67, 0.12, -0.56]])
#标签 (1 表示正面，0 表示负面)
y = torch.tensor([1, 0], dtype=torch.float32)
#定义模型
model = nn.Sequential(
    nn.Linear(3, 5),
    nn.ReLU(),
    nn.Linear(5, 1),
    nn.Sigmoid()
)
#定义损失函数和优化器
criterion = nn.BCELoss()  # 二元交叉熵损失
optimizer = optim.SGD(model.parameters(), lr=0.01)
#训练模型
for epoch in range(100):
    optimizer.zero_grad()  # 清空梯度
    output = model(X).squeeze()
    loss = criterion(output, y)
    loss.backward()  # 反向传播，计算梯度
    optimizer.step()  # 更新参数
    if epoch % 10 == 0:
        print(f'Epoch [{epoch}/100], Loss: {loss.item():.4f}')
#输出结果
predictions = model(X).detach().numpy()
print(f"Predictions: {predictions}")

🔢 （5）输出示例

Epoch [0/100], Loss: 0.6897
Epoch [10/100], Loss: 0.6489
Epoch [20/100], Loss: 0.6215
Epoch [90/100], Loss: 0.5312
Predictions: [0.9123, 0.0897]  # 第一个样本更可能是正面，第二个更可能是负面

🏅 3. 总结


🎯 👉 到这一步，模型已经可以自动学习哪些词是正面情感，哪些词是负面情感了！😎

