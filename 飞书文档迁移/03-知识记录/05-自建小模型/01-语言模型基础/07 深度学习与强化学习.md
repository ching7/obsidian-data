---
created: '2025-03-20 14:14:37'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/OHXsdDEd5oJ6pKx3uvIcdRGknNe
modified: '2025-03-21 14:03:46'
source: feishu
title: 07 æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ 
---

07 æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ 
é‚£æˆ‘ä»¬å°±ä»ä¸€ä¸ªåŸºç¡€çš„ä¸¤å±‚ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ å…¥æ‰‹ï¼Œä½¿ç”¨ Python æ¥å®ç°ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå­¦ä¹ åŠ æ³•å’Œå‡æ³•ã€‚

ğŸ§  ç›®æ ‡ï¼š
æ·±åº¦å­¦ä¹ éƒ¨åˆ†ï¼š 
åˆ›å»ºä¸€ä¸ªä¸¤å±‚çš„ç¥ç»ç½‘ç»œ
è®­ç»ƒç½‘ç»œæ¥å­¦ä¹ åŠ æ³•å’Œå‡æ³•çš„è§„å¾‹
å¼ºåŒ–å­¦ä¹ éƒ¨åˆ†ï¼š 
åœ¨ç¯å¢ƒä¸­å®šä¹‰åŠ æ³•å’Œå‡æ³•ä»»åŠ¡
é€šè¿‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–ç­–ç•¥
è¯¦ç»†æ³¨é‡Šå’ŒåŸç†è§£é‡Š

ğŸ” 1. æ·±åº¦å­¦ä¹ éƒ¨åˆ†ï¼ˆ2 å±‚ç¥ç»ç½‘ç»œï¼‰
ğŸ† ç¥ç»ç½‘ç»œç»“æ„
è¾“å…¥å±‚ï¼ˆ2ä¸ªç¥ç»å…ƒï¼‰ â†’ è¾“å…¥ä¸¤ä¸ªæ•°
éšè—å±‚ï¼ˆ5ä¸ªç¥ç»å…ƒï¼‰ â†’ ReLU æ¿€æ´» 06 æ¿€æ´»å‡½æ•°
å¢åŠ æ­£åˆ™åŒ–
è°ƒæ•´ä¸åŒçš„æ¢¯åº¦ä¼˜åŒ–å™¨
è¾“å‡ºå±‚ï¼ˆ1ä¸ªç¥ç»å…ƒï¼‰ â†’ é¢„æµ‹ç»“æœ

âœ… ä»£ç å®ç°ï¼ˆåŠ æ³•å’Œå‡æ³•çš„æ·±åº¦å­¦ä¹ ï¼‰
ç”¨ PyTorch å®ç°ğŸ‘‡

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


# å®šä¹‰ä¸€ä¸ªç®€å•çš„2å±‚ç¥ç»ç½‘ç»œ
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(2, 5)  # è¾“å…¥å±‚åˆ°éšè—å±‚ (2 -> 5)
        self.output = nn.Linear(5, 1)  # éšè—å±‚åˆ°è¾“å‡ºå±‚ (5 -> 1)
        self.relu = nn.ReLU()  # ReLU æ¿€æ´»å‡½æ•°

    def forward(self, x):
        z1 = self.hidden(x)  # è®¡ç®—éšè—å±‚åŠ æƒå’Œ
        a1 = self.relu(z1)  # é€šè¿‡ ReLU æ¿€æ´»
        z2 = self.output(a1)  # è®¡ç®—è¾“å‡ºå±‚åŠ æƒå’Œ
        return z2, z1, a1  # è¿”å›æ‰€æœ‰ä¸­é—´è®¡ç®—ç»“æœ


# å®šä¹‰æ¨¡å‹
model = SimpleNN()

# å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
criterion = nn.MSELoss()

# å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.1)


# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆåŠ æ³•ï¼‰
def generate_data(num_samples=1000):
    x = np.random.randint(0, 10, (num_samples, 2))
    y_add = x[:, 0] + x[:, 1]
    return torch.tensor(x, dtype=torch.float32), torch.tensor(y_add, dtype=torch.float32)


x_train, y_train_add = generate_data()

# è®­ç»ƒç¥ç»ç½‘ç»œ
for epoch in range(1000):
    optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦

    # å‰å‘ä¼ æ’­
    output, z1, a1 = model(x_train)
    loss = criterion(output.squeeze(), y_train_add)  # è®¡ç®—æŸå¤±

    # æ‰“å°å‰å‘ä¼ æ’­è®¡ç®—è¿‡ç¨‹
    if epoch % 100 == 0:
        print(f"Epoch {epoch + 1}: Loss = {loss.item():.4f}")
        # éšè—å±‚æƒé‡å’Œåç½®å…¬å¼
        hidden_weights = model.hidden.weight.data
        hidden_bias = model.hidden.bias.data
        hidden_formulas = []
        for i in range(5):
            formula = f"z1_{i + 1} = {hidden_weights[i][0].item():.4f} * x1 + {hidden_weights[i][1].item():.4f} * x2 + {hidden_bias[i].item():.4f}"
            hidden_formulas.append(formula)
        print("Hidden layer formulas:")
        for formula in hidden_formulas:
            print(formula)

        # è¾“å‡ºå±‚æƒé‡å’Œåç½®å…¬å¼
        output_weights = model.output.weight.data
        output_bias = model.output.bias.data
        output_formula = f"z2 = {output_weights[0][0].item():.4f} * a1_1 + {output_weights[0][1].item():.4f} * a1_2 + {output_weights[0][2].item():.4f} * a1_3 + {output_weights[0][3].item():.4f} * a1_4 + {output_weights[0][4].item():.4f} * a1_5 + {output_bias[0].item():.4f}"
        print("Output layer formula:")
        print(output_formula)

    # åå‘ä¼ æ’­
    loss.backward()

    # æ‰“å°æ¢¯åº¦ä¿¡æ¯
    if epoch % 100 == 0:
        # éšè—å±‚æƒé‡æ¢¯åº¦å…¬å¼
        hidden_weights_grad = model.hidden.weight.grad
        hidden_weights_grad_formulas = []
        for i in range(5):
            formula = f"dW1_{i+1}_x1 = {hidden_weights_grad[i][0].item():.4f}, dW1_{i+1}_x2 = {hidden_weights_grad[i][1].item():.4f}"
            hidden_weights_grad_formulas.append(formula)
        print("Hidden layer weights gradient formulas:")
        for formula in hidden_weights_grad_formulas:
            print(formula)

        # éšè—å±‚åç½®æ¢¯åº¦å…¬å¼
        hidden_bias_grad = model.hidden.bias.grad
        hidden_bias_grad_formulas = []
        for i in range(5):
            formula = f"dB1_{i+1} = {hidden_bias_grad[i].item():.4f}"
            hidden_bias_grad_formulas.append(formula)
        print("Hidden layer bias gradient formulas:")
        for formula in hidden_bias_grad_formulas:
            print(formula)

        # è¾“å‡ºå±‚æƒé‡æ¢¯åº¦å…¬å¼
        output_weights_grad = model.output.weight.grad
        output_weights_grad_formulas = []
        for i in range(5):
            formula = f"dW2_1_a1_{i+1} = {output_weights_grad[0][i].item():.4f}"
            output_weights_grad_formulas.append(formula)
        print("Output layer weights gradient formulas:")
        for formula in output_weights_grad_formulas:
            print(formula)

        # è¾“å‡ºå±‚åç½®æ¢¯åº¦å…¬å¼
        output_bias_grad = model.output.bias.grad
        print(f"Output layer bias gradient formula: dB2_1 = {output_bias_grad.item():.4f}")

    optimizer.step()  # æ›´æ–°å‚æ•°

# æµ‹è¯•åŠ æ³•æ•ˆæœ
x_test = torch.tensor([[5, 3]], dtype=torch.float32)
predicted, _, _ = model(x_test)
print(f"5 + 3 = {predicted.item():.2f}")

# åœ¨è¿™ä¸ªç¥ç»ç½‘ç»œçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œz1ã€x1ã€x2ã€z2 å’Œ a1_1 ç­‰ç¬¦å·ä»£è¡¨äº†ç¥ç»ç½‘ç»œä¸­ä¸åŒå±‚çš„å˜é‡å’Œè®¡ç®—ç»“æœï¼Œä¸‹é¢ä¸ºä½ è¯¦ç»†è§£é‡Šï¼š

# è¾“å…¥å±‚
# x1 å’Œ x2ï¼šè¿™ä¸¤ä¸ªå˜é‡ä»£è¡¨è¾“å…¥å±‚çš„ç¥ç»å…ƒã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¾“å…¥å±‚æœ‰ 2 ä¸ªç¥ç»å…ƒï¼Œ
# å› ä¸ºåœ¨ SimpleNN ç±»çš„ __init__ æ–¹æ³•é‡Œï¼Œself.hidden = nn.Linear(2, 5) è¡¨ç¤ºè¾“å…¥å±‚çš„ç»´åº¦æ˜¯ 2ã€‚
# è®­ç»ƒæ•°æ®æ˜¯é€šè¿‡ generate_data å‡½æ•°ç”Ÿæˆçš„ï¼Œæ¯æ¬¡è¾“å…¥æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å‘é‡ï¼Œè¿™ä¸¤ä¸ªå…ƒç´ åˆ†åˆ«å¯¹åº” x1 å’Œ x2ã€‚ä¾‹å¦‚ï¼Œå½“è¾“å…¥ [5, 3] æ—¶ï¼Œx1 å°±æ˜¯ 5ï¼Œx2 å°±æ˜¯ 3ã€‚

# éšè—å±‚
# z1ï¼šz1 æ˜¯éšè—å±‚ç¥ç»å…ƒçš„åŠ æƒå’Œï¼Œå®ƒæ˜¯è¾“å…¥å±‚çš„è¾“å‡ºç»è¿‡éšè—å±‚æƒé‡çŸ©é˜µåŠ æƒå¹¶åŠ ä¸Šåç½®åçš„ç»“æœã€‚
# åœ¨ä»£ç ä¸­ï¼Œz1 = self.hidden(x) è¿™ä¸€è¡Œå®ç°äº†è¿™ä¸ªè®¡ç®—ã€‚
# ç”±äºéšè—å±‚æœ‰ 5 ä¸ªç¥ç»å…ƒï¼Œæ‰€ä»¥ z1 å®é™…ä¸Šæ˜¯ä¸€ä¸ªåŒ…å« 5 ä¸ªå…ƒç´ çš„å‘é‡ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º z1_1ã€z1_2ã€z1_3ã€z1_4 å’Œ z1_5ã€‚
# æ¯ä¸ª z1_i éƒ½ç”±è¾“å…¥å±‚çš„ x1 å’Œ x2 ä»¥åŠå¯¹åº”çš„æƒé‡å’Œåç½®è®¡ç®—å¾—åˆ°ï¼Œä¾‹å¦‚ z1_1 = -0.6969 * x1 + -0.2592 * x2 + -0.4995ã€‚
# a1ï¼ša1 æ˜¯éšè—å±‚ç¥ç»å…ƒç»è¿‡æ¿€æ´»å‡½æ•°ï¼ˆè¿™é‡Œæ˜¯ ReLU æ¿€æ´»å‡½æ•°ï¼‰å¤„ç†åçš„è¾“å‡ºã€‚åœ¨ä»£ç ä¸­ï¼Œa1 = self.relu(z1) å®ç°äº†è¿™ä¸ªæ“ä½œã€‚
# åŒæ ·ï¼Œa1 ä¹Ÿæ˜¯ä¸€ä¸ªåŒ…å« 5 ä¸ªå…ƒç´ çš„å‘é‡ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º a1_1ã€a1_2ã€a1_3ã€a1_4 å’Œ a1_5ã€‚

# è¾“å‡ºå±‚
# z2ï¼šz2 æ˜¯è¾“å‡ºå±‚ç¥ç»å…ƒçš„åŠ æƒå’Œï¼Œå®ƒæ˜¯éšè—å±‚çš„è¾“å‡º a1 ç»è¿‡è¾“å‡ºå±‚æƒé‡çŸ©é˜µåŠ æƒå¹¶åŠ ä¸Šåç½®åçš„ç»“æœã€‚
# åœ¨ä»£ç ä¸­ï¼Œz2 = self.output(a1) å®ç°äº†è¿™ä¸ªè®¡ç®—ã€‚è¿™é‡Œè¾“å‡ºå±‚åªæœ‰ 1 ä¸ªç¥ç»å…ƒï¼Œæ‰€ä»¥ z2 æ˜¯ä¸€ä¸ªæ ‡é‡å€¼ã€‚
# æ€»ç»“æ¥è¯´ï¼Œè¿™äº›ç¬¦å·ä»£è¡¨äº†ç¥ç»ç½‘ç»œåœ¨è¿›è¡Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸åŒå±‚çš„è¾“å…¥ã€åŠ æƒå’Œä»¥åŠæ¿€æ´»åçš„è¾“å‡ºï¼Œé€šè¿‡è¿™äº›è®¡ç®—ï¼Œç¥ç»ç½‘ç»œå¯ä»¥ä»è¾“å…¥æ•°æ®ä¸­å­¦ä¹ å¹¶è¾“å‡ºé¢„æµ‹ç»“æœã€‚

ğŸ¯ ğŸ’¡ æ·±åº¦å­¦ä¹ éƒ¨åˆ†è§£é‡Š
è¾“å…¥å±‚ï¼š
 è¾“å…¥ [5, 3]ï¼Œä»£è¡¨ä¸¤ä¸ªæ•°å­—ã€‚
éšè—å±‚ï¼š 
æƒé‡çŸ©é˜µ W1ï¼šå­¦ä¹ å¦‚ä½•å°†è¾“å…¥æ˜ å°„åˆ°ä¸­é—´ç‰¹å¾
æ¿€æ´»å‡½æ•° ReLUï¼šéçº¿æ€§å˜æ¢ï¼Œå¸®åŠ©ç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚æ¨¡å¼
è¾“å‡ºå±‚ï¼š 
æƒé‡çŸ©é˜µ W2ï¼šå­¦ä¹ å¦‚ä½•å°†ä¸­é—´ç‰¹å¾æ˜ å°„åˆ°æœ€ç»ˆç»“æœ
ç›´æ¥è¾“å‡ºé¢„æµ‹å€¼

ğŸ§  ç½‘ç»œå­¦ä¹ å†…å®¹
å­¦ä¹ åˆ°çš„æƒé‡ä¼šè°ƒæ•´ä¸ºï¼š
 
å› ä¸ºåŠ æ³•æ˜¯çº¿æ€§é—®é¢˜ï¼Œç»è¿‡å¤šè½®è®­ç»ƒï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å°†æƒé‡è°ƒæ•´åˆ°æ¥è¿‘ 1ã€‚

ğŸš€ 2. å¼ºåŒ–å­¦ä¹ éƒ¨åˆ†ï¼ˆå­¦ä¹ åŠ æ³•å’Œå‡æ³•ï¼‰
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®©æ¨¡å‹â€œé€šè¿‡å¥–åŠ±â€å­¦ä¹ åŠ å‡æ³•ã€‚
âœ… å¼ºåŒ–å­¦ä¹ å®šä¹‰
çŠ¶æ€ï¼ˆStateï¼‰ = è¾“å…¥çš„ä¸¤ä¸ªæ•°
åŠ¨ä½œï¼ˆActionï¼‰ = æ¨¡å‹è¾“å‡ºï¼ˆåŠ æ³•æˆ–å‡æ³•ï¼‰
å¥–åŠ±ï¼ˆRewardï¼‰ = è¾“å‡ºä¸å®é™…å€¼çš„æ¥è¿‘ç¨‹åº¦
ç­–ç•¥ï¼ˆPolicyï¼‰ = é€šè¿‡æ¢¯åº¦æå‡ï¼Œä½¿è¾“å‡ºæ›´æ¥è¿‘æ­£ç¡®ç»“æœ

âœ… ä»£ç å®ç°ï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ å­¦ä¹ åŠ å‡æ³•ï¼‰
ç”¨ PyTorch + RL å®ç°ğŸ‘‡

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# å®šä¹‰ generate_data å‡½æ•°
def generate_data(num_samples):
    # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®
    state = torch.tensor(np.random.randint(0, 10, (num_samples, 2)), dtype=torch.float32)
    # è®¡ç®—åŠ æ³•ç›®æ ‡
    target_add = state[:, 0] + state[:, 1]
    # è®¡ç®—å‡æ³•ç›®æ ‡
    target_sub = state[:, 0] - state[:, 1]
    return state, target_add, target_sub

class ReinforceModel(nn.Module):
    def __init__(self):
        super(ReinforceModel, self).__init__()
        self.hidden = nn.Linear(2, 5)
        self.output = nn.Linear(5, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.hidden(x))
        x = self.output(x)
        return x

# å®šä¹‰æ¨¡å‹å’Œä¼˜åŒ–å™¨
model = ReinforceModel()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# å®šä¹‰å¥–åŠ±æœºåˆ¶
def compute_reward(pred, target):
    return -torch.abs(pred - target)  # è´Ÿçš„ç»å¯¹å·®å€¼ä½œä¸ºæƒ©ç½š

# è®­ç»ƒå¼ºåŒ–å­¦ä¹ 
for epoch in range(1000):
    state, target_add, target_sub = generate_data(1)  # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ•°æ®
    optimizer.zero_grad()

    # åŠ æ³•å­¦ä¹ 
    prediction = model(state)
    reward = compute_reward(prediction, target_add)

    # è´Ÿçš„æŸå¤±ç”¨äºæ¢¯åº¦æå‡
    loss = -reward  # å¼ºåŒ–å­¦ä¹ å¸Œæœ›æœ€å¤§åŒ–å¥–åŠ±

    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/1000], Reward: {reward.item():.4f}')

# æµ‹è¯•åŠ æ³•æ•ˆæœ
x_test = torch.tensor([[5, 3]], dtype=torch.float32)
predicted = model(x_test).item()
print(f"5 + 3 = {predicted:.2f}")

ğŸ¯ ğŸ’¡ å¼ºåŒ–å­¦ä¹ éƒ¨åˆ†è§£é‡Š
è¾“å…¥çŠ¶æ€ï¼šè¾“å…¥ [5, 3]
è¾“å‡ºåŠ¨ä½œï¼šæ¨¡å‹è¾“å‡ºç»“æœ
å¥–åŠ±æœºåˆ¶ï¼š 
è¾“å‡ºä¸çœŸå®å€¼è¶Šæ¥è¿‘ï¼Œå¥–åŠ±è¶Šå¤§
é€šè¿‡æœ€å¤§åŒ–å¥–åŠ±æ¥ä¼˜åŒ–å‚æ•°
åå‘ä¼ æ’­ï¼šæ ¹æ®å¥–åŠ±æ¥æ›´æ–°æƒé‡

ğŸŒŸ 3. æ·±åº¦å­¦ä¹  vs å¼ºåŒ–å­¦ä¹  åŒºåˆ«


ğŸš€ 4. æ€»ç»“
âœ… æ·±åº¦å­¦ä¹ ï¼šé€šè¿‡ç›‘ç£å­¦ä¹ ç›´æ¥æ‹ŸåˆåŠ æ³•å’Œå‡æ³•çš„æ¨¡å¼
 âœ… å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡â€œå¥–åŠ±â€å¼•å¯¼æ¨¡å‹å­¦ä¹ è§„å¾‹

ğŸ’¡ ğŸ‘‰ æ·±åº¦å­¦ä¹  â†’ å»ºæ¨¡èƒ½åŠ› + å¼ºåŒ–å­¦ä¹  â†’ å†³ç­–èƒ½åŠ› = å¼ºå¤§ AIï¼ ğŸ˜
