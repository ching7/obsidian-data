---
created: '2025-03-20 14:26:08'
feishu_url: https://wk5tnvpfe7.feishu.cn/docx/FpqsdHFXqogXKWx2RsRcJdR9nkb
modified: '2025-03-21 15:31:33'
source: feishu
title: 06 激活函数
---

06 激活函数
好的！ReLU（Rectified Linear Unit，修正线性单元）是神经网络中最常见的激活函数之一。我们来一步步拆解，让它更直观易懂👇

🏆 1. 什么是激活函数？
👉 你可以把神经网络看成是一系列的“过滤器”：
每个神经元都会接收输入 → 进行加权求和 → 输出一个结果
但如果不加“激活函数”，输出的结果只能是“线性的”（即直线）
问题在于：
如果神经网络中全是“线性”操作（加法、乘法），模型只能学会“直线”
实际问题（比如加法、乘法、分类）往往是“非线性”的
激活函数的作用：
 ✅ 引入非线性，使网络可以学习复杂的模式

🎯 2. ReLU 的定义
ReLU 的数学公式是：
 
即：
如果输入 > 0 → 输出等于输入
如果输入 ≤ 0 → 输出为 0

🚀 ReLU 示例
比如你有以下输入：

x = [-2, -1, 0, 1, 2]
经过 ReLU 变换后的输出是：

[0, 0, 0, 1, 2]
👉 负数 → 直接砍掉，输出为 0
 👉 正数 → 直接保留

💡 用生活例子来解释
👉 假设你在和朋友聊天，以下是你们的对话内容：
朋友说：“我刚中了彩票！” → 你很开心 😊（正面反馈，ReLU 输出 > 0）
朋友说：“我昨天丢了钱包。” → 你感到很沮丧 😔（负面反馈，ReLU 输出 = 0）
ReLU 就像一个情绪调节器：
积极的信息 → 直接保留
负面的信息 → 直接屏蔽（变成 0）

🧠 3. 为什么用 ReLU 而不用其他的？
👉 在神经网络中，常见的激活函数有：

✅ ReLU 的优势
计算快： 
ReLU 只需要判断输入是正数或负数 → 速度快
梯度消失问题少： 
Sigmoid 在输入很小时，梯度趋近于 0，导致学习停滞
ReLU 在正区间保持较大的梯度 → 学习更快
让模型更稀疏（Sparsity）： 
ReLU 输出为 0 的情况比较多
这就意味着网络中会有一部分神经元被“关闭” → 网络更轻量、更高效

🎯 用生活例子来解释 ReLU 的优势
👉 把神经网络比喻成一支“篮球队”：
ReLU = 训练方式
每个神经元 = 一个球员
如果用 Sigmoid：
每个球员都要上场，不管表现好不好（即使负值 → 也会被考虑）
结果：球队整体表现可能不理想
如果用 ReLU：
表现不佳的球员（负值）直接替换掉（输出为 0）
只保留正面贡献的球员（正值）
结果：球队整体表现更强 → 模型更有效

🚀 4. 为什么 ReLU 有助于加法学习？
在加法和减法任务中，ReLU 的特点非常有用：
👉 假设我们要学习：
 
输入 = [5, 3]
加权求和后输出可能是正数或负数
通过 ReLU 进行激活： 
如果加权求和是正的（比如 8） → ReLU 输出 = 8（直接保留）
如果加权求和是负的（比如 -2） → ReLU 输出 = 0（丢弃错误贡献）
所以：
ReLU 保留有效的正输出
舍弃错误的负输出
使模型能更好地学习加法和减法的“规律”

🔥 5. ReLU 的局限性
ReLU 并不完美，存在一个问题：
❌ 神经元死亡（Dead Neuron）问题
如果某个神经元在训练过程中始终输出负数 → ReLU 输出为 0
这个神经元在未来训练中将“永久死亡”（因为输出一直是 0）
✅ 解决方法：Leaky ReLU
Leaky ReLU 允许输出为负数时，输出一个小的负值（比如 0.01x）而不是直接 0：
 

🚀 6. 用 ReLU + 加法完整过程示例
涉及矩阵相乘08-矩阵

import torch
import torch.nn as nn

# 定义 ReLU
relu = nn.ReLU()
# 输入：5 和 3
input_data = torch.tensor([[5.0, 3.0]])
# 权重初始化
# 修改 W 的形状为 (2, 5)
W = torch.tensor([[1.0, 0.3, -0.2, 0.5, -0.7], [0.5, 0.7, 0.8, -0.5, 0.9]])
b = torch.tensor([0.1, 0.2, -0.1, 0.3, 0.5])
# 线性加权求和
hidden_output = torch.matmul(input_data, W) + b
print(f"加权求和输出: {hidden_output}")
# 通过 ReLU 进行激活
activated_output = relu(hidden_output)
print(f"ReLU 输出: {activated_output}")
💡 输出解释：
torch.matmul → 线性求和
relu → 负数 → 输出 0；正数 → 保留

🌟 7. 总结


🚀 8. 通俗总结
👉 ReLU 就像是个“淘汰机制”：
表现不好的神经元 → 直接屏蔽（输出为 0）
表现好的神经元 → 直接输出 → 强化
所以，ReLU 让神经网络更聪明、更有效！ 😎
